{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfMjSSoej34Y"
   },
   "source": [
    "# 0. Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yei_JVgIpUmG",
    "outputId": "b48dd386-252e-4312-cfc0-8e4a3f087145"
   },
   "outputs": [],
   "source": [
    "!pip install ydata_profiling --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfFPUXOApwcd",
    "outputId": "31e28556-0865-4880-e71e-8b6b25e66794"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Sistema de archivos y operaciones b\u00e1sicas\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "\n",
    "# Manipulaci\u00f3n de datos y an\u00e1lisis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Visualizaci\u00f3n de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# An\u00e1lisis y preprocesamiento de datos\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Modelado predictivo\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Modelos avanzados de Machine Learning\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Progreso de bucles\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Integraci\u00f3n con Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Datos financieros\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZFL5Cna1B5C"
   },
   "source": [
    "# Definici\u00f3n de Par\u00e1metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDudqspe1G5M"
   },
   "outputs": [],
   "source": [
    "start_date_str = \"2000-01-04\"\n",
    "end_date_str = \"2023-12-31\"\n",
    "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "next_day = end_date + timedelta(days=3)\n",
    "\n",
    "test_size = 0.1\n",
    "test_size_0_2 = 0.2\n",
    "test_val = 0.5\n",
    "random_seed = 42\n",
    "cross_validation = 5\n",
    "n_jobs = -1\n",
    "verbose = 2\n",
    "\n",
    "threshold_non_scaled = 0.5\n",
    "\n",
    "it_start_date = start_date\n",
    "it_end_date = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-o5j0Mo1G-v"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylNiRH39QFU"
   },
   "source": [
    "Par\u00e1metros para la Regresi\u00f3n Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-px5gStp1HCr"
   },
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'fit_intercept': [True, False],  # Si se debe calcular el intercepto\n",
    "    'copy_X': [True, False],  # Si se debe copiar X antes de ajustarlo, evitando que se modifiquen los datos originales\n",
    "    'positive': [True, False],  # Si se deben forzar los coeficientes a ser positivos\n",
    "    'n_jobs': [None, -1]  # N\u00famero de trabajos paralelos para ejecutar (-1 significa utilizar todos los procesadores disponibles)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5peJ7Y0J9Qdd"
   },
   "source": [
    "Par\u00e1metros para el Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oItUeKX1HGG"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(bootstrap=True,\n",
    "                           max_depth=3,\n",
    "                           min_samples_leaf=2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500,\n",
    "                           random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHRjrPGA9Qjs"
   },
   "source": [
    "Par\u00e1metros para el LightGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1HBXeUW1HJZ"
   },
   "outputs": [],
   "source": [
    "num_round = 100\n",
    "\n",
    "# Definir los par\u00e1metros del modelo\n",
    "params_lgmb = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 50,\n",
    "    'reg_alpha': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Definir los hiperpar\u00e1metros que deseas ajustar\n",
    "param_grid_lgmb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suLQiupX9QsN"
   },
   "source": [
    "Par\u00e1metros para el XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjZ6BkK81HM5"
   },
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.02, 0.15, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc7I4KZ8qjHy"
   },
   "source": [
    "# 1. EDA y normalizaci\u00f3n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "N8JBY_txqh-q",
    "outputId": "6cb23826-defe-40f9-d675-49280219960c"
   },
   "outputs": [],
   "source": [
    "# Descargar datos de Monster Beverage Corporation (MNST)\n",
    "data = yf.download(\"MNST\", start=start_date, end=end_date, interval=\"1d\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsDzRcH29bUv"
   },
   "source": [
    "Vamos a calcular la rentabilidad, que ser\u00e1 nuestro target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvQzu54hwzcr"
   },
   "outputs": [],
   "source": [
    "data['Rentabilidad'] = ((data['Close'] - data['Open']) / data['Open']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "8B-aCFwP9pMv",
    "outputId": "2a98ed42-3490-4639-a9f7-f9508c478ec4"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_qTKvcRrp6D",
    "outputId": "945b2b5f-9ba6-49ad-d942-c970506cd964"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "mwRyKsFkvj7d",
    "outputId": "2431a394-d8be-45b9-913e-8b52fe04ed8c"
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgVP8oXfsUxG",
    "outputId": "cd71f9e9-7c43-463b-bbb0-6fe6e4177c06"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "1fe29d9c01f34916be418ee36b15ae72",
      "d3eba19f6cd8476cb5c12af6c8ad87ad",
      "b45c022dec4a43758f5c81ea3526bcca",
      "7779a42e94e74139af91c5a5f03b45c7",
      "3378d709a2d24df78fa8b9d95dc2e055",
      "6627ed8c112f4bc9af0a8320d764af7a",
      "b1865f51b1314cc6896ca8215a492f22",
      "c5d065cd78484946862c289f4b48c67a",
      "25c9430a26e0475fa33574423d3b1ca9",
      "6cf8cc070f4f4677a3ff069a9544d0b2",
      "cec18d0229ba4d4c91dbac3bd86029b3",
      "2041ffc6def946fa80c3f3e70e475e1d",
      "4c2319efc3ad41ff9d321e621d3c80e3",
      "2d9673a48e31480884f63aaca7adb8e3",
      "306d2002785c46f0b16759878429cd87",
      "7502a407af1148b0a5b6dc4ce269db72",
      "e4539406d93e4025a8b6a9e147dda626",
      "3ca852e8cc4e499098d2a97661a994a7",
      "b71943a1484e4ce4997c5e0a2bf0fba5",
      "2137ed1122184bf9bfc4ac2bd6d16815",
      "0faa01159b254acf9c6479c1645110a5",
      "72781b53145a4cc3ba8e13ba115b7eb1",
      "7394e0f8ab404c0c9d4305effb209136",
      "02fe8b607989415aba4cb8ad840318e9",
      "dc89f643bdb14e04bc4cda6048161cc3",
      "67c65759d0794dc09ebf1a4a10237ea8",
      "5758df44f2574deb8c09a8db3b2fc823",
      "00723c56698c424393a45106f03c13db",
      "bdb5efbb6a2d46bea256ed4524615924",
      "5473a32ace024ebea45811b7de3e05cb",
      "dd9699085a104864a10220f246fef89e",
      "ff213b0309d045cd9c3b9c8a2d8c922e",
      "95a1debff34d4cf8a67c88b048e096fc",
      "38ce619ddb8046908f8c33a0366c5701",
      "499780d3b8ec457cae7bd9dab82dc905",
      "db2369a9743244e08c7dcafe194b3d71",
      "6dccd04450b640f7b3baebe1ad61ab3f",
      "14909196c99b4d1396bfc3e0da5e3233",
      "b854e9d5d5ed4c02a7f6fce4f34897c1",
      "b24d14afd317422baf72eda0e86ae542",
      "55f1361c9aea4efdafb6ede4a4121548",
      "ef2fb08966054a1c8ba45317e272550e",
      "49e9a1dad8fd4ed48586044bdc6a3982",
      "da8e3ecbdb774073aa9d713539b3dff5"
     ],
     "height": 197
    },
    "id": "_abbRZ5Sqrnz",
    "outputId": "66c5ed39-4537-45dd-b195-b8b2454bdeda"
   },
   "outputs": [],
   "source": [
    "# EDA: Monster Beverage.\n",
    "\n",
    "features_report = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "profile = ProfileReport(data[features_report])\n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "pcY3pRHWubx3",
    "outputId": "664619ed-a06c-460f-d2f1-8bc99dd47b54"
   },
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "JpDsX5xEsT0y",
    "outputId": "8b8403c3-652b-4ca8-a41f-d4f408973057"
   },
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr()\n",
    "# Mostrar la matriz de correlaci\u00f3n\n",
    "print(\"Matriz de correlaci\u00f3n:\")\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jnq3DJvw0UH"
   },
   "source": [
    "### 1.2. Graficaci\u00f3n de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEFenS8Rw3nT",
    "outputId": "53ad4b96-0b05-4df6-c9bb-fd1746bb41be"
   },
   "outputs": [],
   "source": [
    "def plot_line_chart(ticker):\n",
    "    # Descargar datos normalmente para el ticker proporcionado\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "    selected_columns = data['Close']\n",
    "\n",
    "    # Crear el gr\u00e1fico de l\u00edneas\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=selected_columns.index, y=selected_columns, mode='lines', name=f'Close_{ticker}'))\n",
    "\n",
    "    # Configurar el dise\u00f1o del gr\u00e1fico\n",
    "    fig.update_layout(title=f'Gr\u00e1fico de L\u00edneas para {ticker}',\n",
    "                      xaxis_title='Fecha',\n",
    "                      yaxis_title='Precio',\n",
    "                      template='plotly_dark')\n",
    "\n",
    "    # Mostrar el gr\u00e1fico\n",
    "    fig.show()\n",
    "\n",
    "# Llamar a la funci\u00f3n para el ticker \"MNST\" (Monster)\n",
    "plot_line_chart(\"MNST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEBGCIS1w3uy",
    "outputId": "4599e729-d67b-4aa5-abd1-26c3210cce86"
   },
   "outputs": [],
   "source": [
    "def plot_candlestick_chart(ticker):\n",
    "    # Descargar datos normalmente para el ticker proporcionado\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "\n",
    "    # Crear el gr\u00e1fico de velas\n",
    "    fig = go.Figure(data=[go.Candlestick(x=data.index,\n",
    "                                          open=data['Open'],\n",
    "                                          high=data['High'],\n",
    "                                          low=data['Low'],\n",
    "                                          close=data['Close'])])\n",
    "\n",
    "    # Configurar el dise\u00f1o del gr\u00e1fico\n",
    "    fig.update_layout(title=f'Gr\u00e1fico de Velas para {ticker}',\n",
    "                      xaxis_title='Fecha',\n",
    "                      yaxis_title='Precio',\n",
    "                      template='plotly_dark')\n",
    "\n",
    "    # Mostrar el gr\u00e1fico\n",
    "    fig.show()\n",
    "\n",
    "# Llamar a la funci\u00f3n para el ticker \"MNST\" (Monster)\n",
    "plot_candlestick_chart(\"MNST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ9Uc_fDviNX"
   },
   "source": [
    "### 1.3. Normalizaci\u00f3n y correspondiente EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTjr2CL-J5LO"
   },
   "source": [
    "Normalizamos las columnas de volumen y rentabilidad para tener valores en un rango entre [-1,1].\n",
    "Lo hacemos para ayudar a mejorar la interpretaci\u00f3n y el rendimiento de los modelos de aprendizaje autom\u00e1tico, especialmente en el contexto de la rentabilidad de acciones, donde se espera una amplia gama de valores positivos y negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhAWPeEcKhSY"
   },
   "outputs": [],
   "source": [
    "# Normalizar la columna \"Volume y Rentabilidad\"\n",
    "data['Volume_normalized'] = scaler.fit_transform(data[['Volume']])\n",
    "\n",
    "data['Rentabilidad_normalized'] = scaler.fit_transform(data[['Rentabilidad']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "e546dfe4feee4933b2eff482d1a97c6b",
      "01e4ae272f384be1baed03394bde1e01",
      "b94d4a5718a94d9ea13926533fcb99f9",
      "eef741425dfb41c291db815a47219cdb",
      "fa4a9aef8fd54909be8ceed7b6eb7468",
      "e928a26784b94c488d7982766a597554",
      "740877c702504d378fcc88bfb93b1522",
      "10c0a4cba0d54dd98b048cd714a4560a",
      "fc07e15daa9047c0a3f16759608ef91d",
      "0a7a3407d77d4007918decf9770305b3",
      "97f72a6f34094c71b7aac4850a1b24d5",
      "1578193e62374b8596a30b6686198ac9",
      "5dcaef2f7e734934ab8f793f04ddb8e4",
      "cd68e680b3614cd29346f889729b912c",
      "24d744cc0fc747aa9ca1fbb8458e2cec",
      "ef6184aa5e02470d9430a093f1cf805b",
      "2f4a0861a65f4b2ab26f7bf1577297ed",
      "dc45beec0ff5419d9a847ec4a96af63c",
      "3493559acb1346b8a3bb6c42f8741550",
      "abed477f863d4d1999e40b7435195b16",
      "16f21071b0c54af99492cea14b24d0cc",
      "edac3f6c62fe4a00898e08412330cf93",
      "6481bf6784a24e71b98e5aae8382eb0d",
      "d3eea38e480042029a270079358558f7",
      "00db81425f58436c97c260626d2b6d51",
      "4aa76a46ac964f44aebf4788f55186d2",
      "e487987668424e97a1fe0380ba66970d",
      "5938e1d1aa514ea7acc8f910854121c4",
      "a55393ab4ead408ea901632c061ef009",
      "a3ce1776559f47fd873da64c025fc4f9",
      "ea8c7c4cf8ce4c7bae70a7183a5396ba",
      "dfa8e0f453bc441ebbe8450faa33c9aa",
      "60bd5dff1ae040af87da952eb704bb84",
      "b30622f83b4c41f5bbb0d0b46f347876",
      "011c83e5ccf04979bc2840f777c32acc",
      "45aa512486e44c0ab2fe6010e225887b",
      "ae492ca2884e49d6aca90ad4014921da",
      "8da61eaa2e45430f8877a05ef827fd43",
      "76c8f7421e004150b6d9cc9480a5e4f8",
      "c78d770be72246c3b78177ba0ad02dd8",
      "5fa6d3b8e25c4fd2ae7a75f46a6b73ca",
      "4d4faeea04d748e795951820d130ae79",
      "75e820479d0a4f0f9dd28f32c2d0d190",
      "35c43b25b6a04b1ea525e04ff2aca7a2"
     ],
     "height": 1000
    },
    "id": "PSgQijs9vKoM",
    "outputId": "76fcd40f-59f3-458e-96f2-7802b66a0042"
   },
   "outputs": [],
   "source": [
    "# EDA MODELO ESCALADO (PROFILE REPORT CON TODAS LAS COLUMNAS)\n",
    "\n",
    "features_report = ['Open', 'High', 'Low', 'Close','Volume','Rentabilidad','Volume_normalized','Rentabilidad_normalized']\n",
    "profile = ProfileReport(data[features_report])\n",
    "profile.to_file(\"output.html\")\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds5p_HRvmYHB"
   },
   "source": [
    "# 2. Definir Target y dividir en entrenamiento, test y validaci\u00f3n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81aFXeBrp1Re"
   },
   "source": [
    "El target ser\u00e1 la rentabilidad diaria de cada acci\u00f3n, para ello la hemos calculado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_rbsc6wmpBA",
    "outputId": "abea20a2-7a7f-4461-a9ef-72db66a6d2ff"
   },
   "outputs": [],
   "source": [
    "# Seleccionar los datos para el IT\n",
    "df_it = data.loc[(data.index >= it_start_date) & (data.index <= it_end_date)]\n",
    "\n",
    "# Dividir el DataFrame en caracter\u00edsticas (X) y la variable objetivo (y)\n",
    "X = df_it.drop([\"Rentabilidad\", \"Rentabilidad_normalized\" ,\"Close\",\"Volume\"], axis=1)\n",
    "y = df_it[\"Rentabilidad_normalized\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "# Dividir los datos temporales en conjuntos de validaci\u00f3n y prueba\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_val, random_state=random_seed)\n",
    "\n",
    "# Imprimir el n\u00famero de filas en cada conjunto\n",
    "print(\"IT DataSet has {} rows\".format(len(df_it)))\n",
    "print(\"X_train has {} rows\".format(len(X_train)))\n",
    "print(\"X_val has {} rows\".format(len(X_val)))\n",
    "print(\"X_test has {} rows\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a8d-DEbB6xV"
   },
   "source": [
    "Observamos si en nuestros df creados tenemos valores nan ya que no es recomendable tenerlos en la regresi\u00f3n lineal ni en el random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxjrFVHVuIWg",
    "outputId": "a4532c4e-9d2b-4d70-f5d7-2341eaef6201"
   },
   "outputs": [],
   "source": [
    "nan_values = df_it.isna().any().any()\n",
    "\n",
    "if nan_values:\n",
    "    count_nan = df_it.isna().sum().sum()  # Suma de todos los valores NaN\n",
    "    print(f\"Tu DataFrame 'data' contiene {count_nan} valores NaN.\")\n",
    "else:\n",
    "    print(\"Tu DataFrame 'data' no contiene valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLGBfd_cRu5A"
   },
   "source": [
    "# 3. COMPARACION DE MODELOS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FShgZqwzVKvO"
   },
   "source": [
    "Para nuestro estudio, vamos a realizar una comparacion del MSE entre los modelos de Regresio Lineal, Random Forest y LightGMB, nos quedaremos con el que el error sea menor y nos aporte mayor fiabilidad al estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0AdWNpMzTxZ"
   },
   "source": [
    "## 3.1. Regresi\u00f3n Lineal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr1lUeqD0DSp",
    "outputId": "ad25338e-c6d8-4517-ab2d-a35c67f4a9e3"
   },
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQGOD2pzpqWR",
    "outputId": "00b45f89-f674-49f3-fe36-6ba26549cd9b"
   },
   "outputs": [],
   "source": [
    "list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2_08hhipu35",
    "outputId": "5b54ffdc-be42-4ce9-a194-fa8244791487"
   },
   "outputs": [],
   "source": [
    "list(y.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0gHFEoKJ3bz"
   },
   "source": [
    "## 3.1. REGRESION LINEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "oQ8a936Tzaou",
    "outputId": "4a5ea59e-c157-4239-9688-f295c3cca8d3"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de regresi\u00f3n lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "# print(f'R-squared: {r2}') no lo usamos\n",
    "plt.hist(y_test - y_pred) # vemos que esta centrado en el 0, por lo que podemos aceptarlo\n",
    "\n",
    "plt.title(\"Distribuci\u00f3n de Residuos del Modelo de Regresi\u00f3n Lineal\", fontsize=10)\n",
    "plt.xlabel(\"Residuos\", fontsize=10)\n",
    "plt.ylabel(\"Frecuencia\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "eZW8xu_q37jE",
    "outputId": "fce78bad-e775-48ee-c9a8-71c79a3d3196"
   },
   "outputs": [],
   "source": [
    "# Seleccionar las caracter\u00edsticas que deseas visualizar\n",
    "features = ['Open', 'High', 'Low','Volume_normalized']\n",
    "\n",
    "# Crear una figura y subtramas\n",
    "fig, axes = plt.subplots(nrows=len(features), ncols=1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Iterar sobre cada caracter\u00edstica y crear un gr\u00e1fico para ella\n",
    "for i, feature in enumerate(features):\n",
    "    # Seleccionar la caracter\u00edstica y ordenar los valores\n",
    "    X_feature = X_test[feature]\n",
    "    sorted_indices = X_feature.argsort()\n",
    "    X_feature_sorted = X_feature.iloc[sorted_indices]\n",
    "    y_test_sorted = y_test.iloc[sorted_indices]\n",
    "    y_pred_sorted = y_pred[sorted_indices]\n",
    "\n",
    "    # Graficar la regresi\u00f3n lineal y los datos de prueba para la caracter\u00edstica actual\n",
    "    axes[i].scatter(X_feature_sorted, y_test_sorted, color='black', label='Datos de prueba')\n",
    "    axes[i].plot(X_feature_sorted, y_pred_sorted, color='blue', linewidth=3, label='Regresi\u00f3n Lineal')\n",
    "    axes[i].set_ylabel('Rentabilidad_normalized')\n",
    "    axes[i].set_title('Rentabilidad_normalized vs. ' + feature)\n",
    "    axes[i].legend()\n",
    "\n",
    "    # Aplicar zoom en el eje Y\n",
    "    axes[i].set_ylim(-0.3, -0.1)\n",
    "\n",
    "# Ajustar el espacio entre subtramas\n",
    "plt.tight_layout()\n",
    "plt.xlabel('D\u00edas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YQD8CJ7xtL5"
   },
   "source": [
    "Ploteamos Independientemente el Volumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEH7zN8n5Nks"
   },
   "outputs": [],
   "source": [
    "# Seleccionar la caracter\u00edstica 'Volume'\n",
    "feature = 'Volume_normalized'\n",
    "\n",
    "# Seleccionar la caracter\u00edstica y ordenar los valores\n",
    "X_feature = X_test[feature]\n",
    "sorted_indices = X_feature.argsort()\n",
    "X_feature_sorted = X_feature.iloc[sorted_indices]\n",
    "y_test_sorted = y_test.iloc[sorted_indices]\n",
    "y_pred_sorted = y_pred[sorted_indices]\n",
    "\n",
    "# Visualizar el modelo lineal y los datos de prueba\n",
    "plt.scatter(X_feature_sorted, y_test_sorted, color='black', label='Datos de prueba')\n",
    "plt.plot(X_feature_sorted, y_pred_sorted, color='blue', linewidth=3, label='Regresi\u00f3n Lineal')\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel('Rentabilidad')\n",
    "plt.title('Regresi\u00f3n Lineal: Rentabilidad vs. ' + feature)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4__VeMywxqHr"
   },
   "source": [
    "Vamos a hacer una mascara para eliminar un outlier, con la condicion de que el volumen sea menor a 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhJFTSoSwjpf"
   },
   "outputs": [],
   "source": [
    "# Seleccionar la caracter\u00edstica 'Volume_normalized'\n",
    "feature = 'Volume_normalized'\n",
    "\n",
    "# Aplicar una m\u00e1scara para eliminar los outliers en el rango de -1 a -0.93\n",
    "mask = ((X_test[feature] >= -1) & (X_test[feature] < -0.93))\n",
    "X_feature_masked = X_test[feature][mask]\n",
    "y_test_masked = y_test[mask]\n",
    "y_pred_masked = y_pred[mask]\n",
    "\n",
    "# Visualizar el modelo lineal y los datos de prueba despu\u00e9s de aplicar la m\u00e1scara\n",
    "plt.scatter(X_feature_masked, y_test_masked, color='black', label='Datos de prueba')\n",
    "plt.plot(X_feature_masked, y_pred_masked, color='blue', linewidth=3, label='Regresi\u00f3n Lineal')\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvIq9nhUeyAI"
   },
   "source": [
    "### GRIDSEARCH CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrRElVFje1Tj"
   },
   "outputs": [],
   "source": [
    "# Inicializa GridSearchCV con el modelo y los hiperpar\u00e1metros definidos\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_lr, scoring='neg_mean_squared_error', cv=cross_validation)\n",
    "\n",
    "# Realiza el ajuste para encontrar los mejores hiperpar\u00e1metros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtiene los mejores hiperpar\u00e1metros encontrados\n",
    "best_params_lr = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cNTsBkefLNm"
   },
   "outputs": [],
   "source": [
    "best_params_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MiUozRIfMsF"
   },
   "outputs": [],
   "source": [
    "# Utiliza los mejores hiperpar\u00e1metros para inicializar el modelo\n",
    "model = LinearRegression(**best_params_lr)\n",
    "\n",
    "# Entrena el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Eval\u00faa el modelo\n",
    "mse_test_RL_GS = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse_test_RL_GS}')\n",
    "plt.hist(y_test - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k0PP6w75xAu"
   },
   "source": [
    "### Validaci\u00f3n Cruzada Regresi\u00f3n Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsFAn-ew6RbO"
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de regresi\u00f3n lineal con validaci\u00f3n cruzada\n",
    "model = LinearRegression()\n",
    "\n",
    "# Realizar validaci\u00f3n cruzada\n",
    "scores = cross_val_score(model, X, y, cv=cross_validation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calcular el promedio del error cuadr\u00e1tico medio (MSE) negativo\n",
    "mean_mse_RL_KV = -scores.mean()\n",
    "\n",
    "# Calcular el promedio del coeficiente de determinaci\u00f3n (R-squared)\n",
    "mean_r2_RL = scores.mean()\n",
    "\n",
    "print(f'Mean Squared Error: {mean_mse_RL_KV}')\n",
    "print(f'Mean R-squared: {mean_r2_RL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvE_dProkz6A"
   },
   "source": [
    "### Valor Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er1s_8cQk5Fe"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos\n",
    "residuals_test0 = y_test - y_pred\n",
    "\n",
    "# Imprimir los primeros 10 valores residuales\n",
    "print(\"Residuals:\")\n",
    "residuals_test0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnLpC0NpwDs5"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos\n",
    "residuals_test0 = y_test - y_pred\n",
    "\n",
    "# Crear el histograma de los residuos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals_test0, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# A\u00f1adir t\u00edtulo y etiquetas\n",
    "plt.title('Histograma de Residuos del Modelo de Regresi\u00f3n Lineal')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIvYQcUwy6Hj"
   },
   "source": [
    "## 3.2. Random Forest\n",
    "Aunque el RF se especializa en problemas de clasificaci\u00f3n, nosotros tenemos un problema de regresi\u00f3n, donde tambi\u00e9n nos puede ser \u00fatil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ8d-9MW3vpo"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo de Random Forest con los par\u00e1metros proporcionados\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9TnE00HXL8M"
   },
   "source": [
    "### GRIDSEARCH CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGBj4Ukewmhs"
   },
   "outputs": [],
   "source": [
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid_rf,\n",
    "                           cv=cross_validation, n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "# Ejecutar GridSearchCV en los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores par\u00e1metros\n",
    "print(\"Mejores par\u00e1metros encontrados:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgbUEoM7DfZM"
   },
   "outputs": [],
   "source": [
    "# Realizar predicciones en los conjuntos de entrenamiento y prueba\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadr\u00e1tico medio para cada conjunto\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test_RF_GS = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci\u00f3n (R^2) para cada conjunto\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"M\u00e9tricas del conjunto de entrenamiento:\")\n",
    "print(\"Error cuadr\u00e1tico medio (MSE):\", mse_train)\n",
    "print(\"Coeficiente de determinaci\u00f3n (R^2):\", r2_train)\n",
    "\n",
    "print(\"\\nM\u00e9tricas del conjunto de prueba:\")\n",
    "print(\"Error cuadr\u00e1tico medio (MSE):\", mse_test_RF_GS)\n",
    "print(\"Coeficiente de determinaci\u00f3n (R^2):\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBYLa227IREz"
   },
   "source": [
    "Aqu\u00ed vamos a visualizar nuestras m\u00e9tricas vs los valores reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUfVfx3UIUKk"
   },
   "outputs": [],
   "source": [
    "# Crear un rango de \u00edndices para los puntos de datos\n",
    "indices = range(len(y_test))\n",
    "\n",
    "# Graficar las predicciones vs los valores reales en el conjunto de prueba\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices, y_test, color='red', label='Valores reales')\n",
    "plt.plot(indices, y_pred, color='blue', label='Predicciones')\n",
    "plt.xlabel('\u00cdndices de muestra')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HmaKbHqwcEa"
   },
   "source": [
    "###  Validaci\u00f3n cruzada Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvn_DykSweSj"
   },
   "outputs": [],
   "source": [
    "# Realizar validaci\u00f3n cruzada para MSE\n",
    "cv_scores_mse = cross_val_score(rf, X_train, y_train, cv=cross_validation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convertir los puntajes negativos de MSE a positivos y calcular el promedio\n",
    "mse_scores = -cv_scores_mse\n",
    "mean_mse_RF_KV = mse_scores.mean()\n",
    "\n",
    "# Imprimir el resultado del MSE\n",
    "print(f'Mean Squared Error (MSE) after 5-fold cross validation: {mean_mse_RF_KV}')\n",
    "\n",
    "# Realizar validaci\u00f3n cruzada para R2\n",
    "cv_scores_r2 = cross_val_score(rf, X_train, y_train, cv=cross_validation, scoring='r2')\n",
    "\n",
    "# Calcular el promedio del coeficiente de determinaci\u00f3n R2\n",
    "mean_r2_RF = cv_scores_r2.mean()\n",
    "\n",
    "# Imprimir el resultado de R2\n",
    "print(f'Mean R-squared (R2) after 5-fold cross validation: {mean_r2_RF}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHx3hLCHl5cn"
   },
   "source": [
    "### Valor Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyoexS9Sl7kw"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos en el conjunto de entrenamiento\n",
    "# residuals_train = y_train - y_train_pred\n",
    "\n",
    "# Calcular los residuos en el conjunto de prueba\n",
    "residuals_test1 = y_test - y_test_pred\n",
    "\n",
    "# Imprimir los primeros 10 valores residuales del conjunto de prueba\n",
    "print(\"Residuals del conjunto de prueba:\")\n",
    "print(residuals_test1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRgVxIIhwWuo"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos en el conjunto de prueba\n",
    "# residuals_test1 = y_test - y_test_pred\n",
    "\n",
    "# Crear histograma de los residuos en el conjunto de prueba\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals_test1, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# A\u00f1adir t\u00edtulo y etiquetas\n",
    "plt.title('Histograma de Residuos (Conjunto de Prueba) del Random Forest')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNUtENiH0004"
   },
   "source": [
    "## 3.3. LightGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4KJnP8G0xxr"
   },
   "outputs": [],
   "source": [
    "# Crear un conjunto de datos LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Entrenar el modelo LightGBM\n",
    "bst = lgb.train(params_lgmb, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mse_test_GMB_GS = mean_squared_error(y_test, y_pred)\n",
    "print(\"Error cuadr\u00e1tico medio (MSE):\", mse_test_GMB_GS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6RFrKN61C1h"
   },
   "source": [
    "### Gridsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD87oNhl1Jtx"
   },
   "outputs": [],
   "source": [
    "# Crear un modelo LightGBM\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "\n",
    "# Crear un objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid_lgmb, scoring='neg_mean_squared_error', cv=cross_validation)\n",
    "\n",
    "# Realizar la b\u00fasqueda en la cuadr\u00edcula utilizando los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03S6vrgT6IAg"
   },
   "outputs": [],
   "source": [
    " grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTT4mzIBACk1"
   },
   "source": [
    "### Validaci\u00f3n Cruzada LightGMB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOGZJS3kAF86"
   },
   "outputs": [],
   "source": [
    "# Crear un conjunto de datos LightGBM para el conjunto de entrenamiento\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Definir los par\u00e1metros del modelo. Ya est\u00e1n definidos arriba. Linea 19.\n",
    "''' params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 50,\n",
    "    'reg_alpha': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1  # Desactiva todas las advertencias (warnings)\n",
    "}\n",
    " '''\n",
    "# Crear el modelo LightGBM\n",
    "model = lgb.LGBMRegressor(**params_lgmb)\n",
    "\n",
    "# Realizar la validaci\u00f3n cruzada para MSE\n",
    "cv_scores_mse = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convertir los puntajes negativos de MSE a positivos y calcular el promedio\n",
    "mse_scores = -cv_scores_mse\n",
    "mean_mse_GMB_KV = mse_scores.mean()\n",
    "\n",
    "# Imprimir el resultado del MSE\n",
    "print(f'Mean Squared Error (MSE) after 5-fold cross validation: {mean_mse_GMB_KV}')\n",
    "\n",
    "# Realizar la validaci\u00f3n cruzada para R2\n",
    "cv_scores_r2 = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring='r2')\n",
    "\n",
    "# Calcular el promedio del coeficiente de determinaci\u00f3n R2\n",
    "mean_r2_GMB = cv_scores_r2.mean()\n",
    "\n",
    "# Imprimir el resultado de R2\n",
    "print(f'Mean R-squared (R2) after 5-fold cross validation: {mean_r2_GMB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOw1_RzRlvFH"
   },
   "source": [
    "### Valor Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvDEbttQlw32"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos en el conjunto de entrenamiento\n",
    "# residuals_train = y_train - y_train_pred\n",
    "\n",
    "# Calcular los residuos en el conjunto de prueba\n",
    "residuals_test2 = y_test - y_pred\n",
    "\n",
    "# Imprimir los primeros 10 valores residuales del conjunto de prueba\n",
    "print(\"Residuals del conjunto de prueba:\")\n",
    "print(residuals_test2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ueUI3mJw6ax"
   },
   "outputs": [],
   "source": [
    "# Calcular los residuos en el conjunto de prueba\n",
    "residuals_test2 = y_test - y_pred\n",
    "\n",
    "# Crear histograma de los residuos en el conjunto de prueba\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals_test2, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# A\u00f1adir t\u00edtulo y etiquetas\n",
    "plt.title('Histograma de Residuos (Conjunto de Prueba) de Light GMB')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxSnA0R52qv"
   },
   "source": [
    "## 3.4.XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EFaDJfn5_mh"
   },
   "outputs": [],
   "source": [
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir los valores de rentabilidad normalizados en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"El Mean Squared Error (MSE) es:\", mse)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sxuHzRM6aCz"
   },
   "source": [
    "### 3.4.1 grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFpYRh2d6j9m"
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_xgb, cv=verbose, n_jobs=n_jobs, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar la b\u00fasqueda de cuadr\u00edcula en los datos de entrenamiento con una barra de progreso\n",
    "with tqdm(total=len(param_grid_xgb['n_estimators']) * len(param_grid_xgb['max_depth']) * len(param_grid_xgb['learning_rate']) * len(param_grid_xgb['subsample']) * len(param_grid_xgb['colsample_bytree']) * len(param_grid_xgb['gamma'])) as pbar:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Obtener los mejores hiperpar\u00e1metros encontrados\n",
    "best_params_xgb = grid_search.best_params_\n",
    "print(\"Los mejores hiperpar\u00e1metros encontrados son:\", best_params_xgb)\n",
    "\n",
    "# Utilizar el modelo con los mejores hiperpar\u00e1metros para hacer predicciones\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwbYcs7K7H1m"
   },
   "source": [
    "### 3.4.3. validacion cruzada\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgVVVEeK7PHL"
   },
   "outputs": [],
   "source": [
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor(**best_params_xgb)\n",
    "\n",
    "# Realizar la validaci\u00f3n cruzada\n",
    "mse_scores = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convertir los puntajes de MSE negativos a positivos\n",
    "mse_scores = -mse_scores\n",
    "\n",
    "# Calcular el MSE promedio y la desviaci\u00f3n est\u00e1ndar\n",
    "mean_mse_XGB_KV = mse_scores.mean()\n",
    "\n",
    "print(\"Resultados de validaci\u00f3n cruzada:\")\n",
    "print(\"Mean Squared Error (MSE) promedio:\", mean_mse_XGB_KV)\n",
    "\n",
    "mse_test_XGB_GS = mean_squared_error(y_test, y_pred)\n",
    "print(\"El Mean Squared Error (MSE) utilizando los mejores hiperpar\u00e1metros es:\", mse_test_XGB_GS)\n",
    "\n",
    "'''\n",
    "\n",
    "# Crear el modelo LightGBM\n",
    "model = lgb.LGBMRegressor(**params_lgmb)\n",
    "\n",
    "# Realizar la validaci\u00f3n cruzada para MSE\n",
    "cv_scores_mse = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convertir los puntajes negativos de MSE a positivos y calcular el promedio\n",
    "mse_scores = -cv_scores_mse\n",
    "mean_mse_GMB_KV = mse_scores.mean()\n",
    "\n",
    "# Imprimir el resultado del MSE\n",
    "print(f'Mean Squared Error (MSE) after 5-fold cross validation: {mean_mse_GMB_KV}')\n",
    "\n",
    "# Realizar la validaci\u00f3n cruzada para R2\n",
    "cv_scores_r2 = cross_val_score(model, X_train, y_train, cv=cross_validation, scoring='r2')\n",
    "\n",
    "# Calcular el promedio del coeficiente de determinaci\u00f3n R2\n",
    "mean_r2_GMB = cv_scores_r2.mean()\n",
    "\n",
    "# Imprimir el resultado de R2\n",
    "print(f'Mean R-squared (R2) after 5-fold cross validation: {mean_r2_GMB}') '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2D1JZyKBWuI"
   },
   "source": [
    "### 3.4.4. valor residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTQKwD_1BdlI"
   },
   "outputs": [],
   "source": [
    "# Dividir el DataFrame completo en caracter\u00edsticas (X) y la variable objetivo (y)\n",
    "# X = df_it.drop([\"Rentabilidad\",\"Rentabilidad_normalized\",\"Close\",\"Volume\"], axis=1)\n",
    "# y = df_it[\"Rentabilidad_normalized\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "'''\n",
    "# Crear y entrenar el modelo XGBoost con los mejores hiperpar\u00e1metros encontrados\n",
    "best_model = xgb.XGBRegressor(learning_rate=best_params_xgb['learning_rate'],\n",
    "                               max_depth=best_params_xgb['max_depth'],\n",
    "                               n_estimators=best_params_xgb['n_estimators'],\n",
    "                               subsample=best_params_xgb['subsample'],\n",
    "                               gamma=best_params_xgb['gamma'],\n",
    "                               colsample_bytree=best_params_xgb['colsample_bytree'])\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    " '''\n",
    "# Calcular los residuos\n",
    "residuals_test3 = y_test - y_pred\n",
    "print(residuals_test3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSqrrxr8OOv5"
   },
   "outputs": [],
   "source": [
    "# Crear histograma de los residuos en el conjunto de prueba (residuals_test3)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals_test3, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# A\u00f1adir t\u00edtulo y etiquetas\n",
    "plt.title('Histograma de Residuos (Conjunto de Prueba) de XGBoost')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahLDSVutTad7"
   },
   "source": [
    "# 4. RESULTADO DE LA COMPARACI\u00d3N DE LOS MODELOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "0kaswgX4eJ6c",
    "outputId": "8d7c7844-4e57-4d77-8294-4f8ff4d752e0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "En el LightGMB, el MSE despues de la validacion cruzada (0.004012523386813407) es menor que cuando realizamos el gridsearch (0.005202726655911077), por lo que nos quedariamos\n",
    "con el de Validaci\u00f3n Cruzada.\n",
    "'''\n",
    "print(f'MSE RL GRIDSEARCH {mse_test_RL_GS}')\n",
    "print(f'MSE RL VALIDACION {mean_mse_RL_KV}')\n",
    "print()\n",
    "'''\n",
    "En el LightGMB, el MSE despues de la validacion cruzada (0.004068916719098916) es menor que cuando realizamos el gridsearch (0.005617624942031694), por lo que nos quedariamos\n",
    "con el de Validaci\u00f3n Cruzada.\n",
    "'''\n",
    "print(f'MSE RF GRIDSEARCH {mse_test_RF_GS}')\n",
    "print(f'MSE RF VALIDACION {mean_mse_RF_KV}')\n",
    "print()\n",
    "'''\n",
    "En el LightGMB, el MSE despues de la validacion cruzada (0.0038537084923566667) es menor que cuando realizamos el gridsearch (0.004761497669264618), por lo que nos quedariamos\n",
    "con el de Validaci\u00f3n Cruzada.\n",
    "'''\n",
    "print(f'MSE GMB GRIDSEARCH {mse_test_GMB_GS}')\n",
    "print(f'MSE GMB VALIDACION {mean_mse_GMB_KV}')\n",
    "print()\n",
    "'''\n",
    "En el XGBoost, el MSE despues de la validacion cruzada (0.004844710340393013) es menor que cuando realizamos el gridsearch (0.0050089901096838215), por lo que nos quedariamos\n",
    "con el de Validaci\u00f3n Cruzada.\n",
    "'''\n",
    "print(f'MSE XGB GRIDSEARCH {mse_test_XGB_GS}')\n",
    "print(f'MSE XGB VALIDACION {mean_mse_XGB_KV}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4kxptMdBsaV"
   },
   "outputs": [],
   "source": [
    "print(f'MSE RL GRIDSEARCH {mse_test_RL_GS}')\n",
    "print()\n",
    "print(f'MSE RF GRIDSEARCH {mse_test_RF_GS}')\n",
    "print()\n",
    "print(f'MSE GMB GRIDSEARCH {mse_test_GMB_GS}')\n",
    "print()\n",
    "print(f'MSE XGB GRIDSEARCH {mse_test_XGB_GS}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwKcQV1IyeWN"
   },
   "source": [
    "Nos quedar\u00edamos con el modelo de LightGMB despues de realizar la validaci\u00f3n cruzada, es el que menor MSE nos proporciona.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0xB-Hj0oF_P"
   },
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmPlG0dZyjIc"
   },
   "source": [
    "Debemos realizar plots de los mejores resultados de cada modelo que son los de validaci\u00f3n cruzada:\n",
    "- mean_mse_RL_KV\n",
    "- mean_mse_RF_KV\n",
    "- mean_mse_GMB_KV\n",
    "- mean_mse_XGB_KV\n",
    "\n",
    "Al ser el error, tenemos que quedarnos con el que menor cantidad d\u00e9, esto significa que nos quedaremos con la barra m\u00e1s peque\u00f1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6blU-FM8ysQr"
   },
   "source": [
    "Habr\u00eda que a\u00f1adir otro resultado, el cual ser\u00eda el resultado del modelo nuevo que nos indicar\u00eda hacer Roberto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4_TvhjGywuo"
   },
   "source": [
    "Para aumentar la diferencia vertical en el eje y y resaltar las disparidades entre los valores, aplicamos una transformaci\u00f3n logar\u00edtmica al eje y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "got9qxd-vUJS"
   },
   "outputs": [],
   "source": [
    "# Valores de las variables\n",
    "means = [mse_test_RF_GS, mse_test_RL_GS, mse_test_GMB_GS, mse_test_XGB_GS]\n",
    "\n",
    "labels = ['Random Forest', 'Regresi\u00f3n Lineal', 'LightGBM', 'XGBoost']\n",
    "\n",
    "# Elevar al cuadrado los valores para amplificar las diferencias\n",
    "means_squared = [val ** 2 for val in means]\n",
    "\n",
    "# Crear el diagrama de barras con los valores al cuadrado\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, means_squared, color=['red', 'yellow', 'blue', 'green'])\n",
    "plt.title('Comparaci\u00f3n de MSE promedio entre modelos (Squared)')\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('MSE promedio (Squared)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4Rvm7JXy14f"
   },
   "source": [
    "Ploteo del Valor Residual de los 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTQX7Ds4pbxv"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotear los residuos para cada conjunto de prueba con colores transparentes, sin relleno y con grosor de l\u00ednea aumentado\n",
    "plt.hist(residuals_test0, bins=20, color='red', label='Regresi\u00f3n Lineal', alpha=0.3, histtype='step', linewidth=3)\n",
    "plt.hist(residuals_test1, bins=20, color='yellow', label='Random Forest', alpha=0.3, histtype='step', linewidth=3)\n",
    "plt.hist(residuals_test2, bins=20, color='blue', label='LightGBM', alpha=0.3, histtype='step', linewidth=3)\n",
    "plt.hist(residuals_test3, bins=20, color='green', label='XGBoost', alpha=0.3, histtype='step', linewidth=3)\n",
    "\n",
    "plt.title('Residuos del conjunto de prueba a lo largo del tiempo')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UPZZgiL4QDR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8cw6K5hjZF4"
   },
   "source": [
    "# 5. MODELO FINAL (INCLUYE BINARIZACION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjkow9wM6Vk1"
   },
   "source": [
    "## Descarga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do2wzN4aJVMt"
   },
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V9AIyPhptlf"
   },
   "outputs": [],
   "source": [
    "columnas_binarizar = ['Open', 'High', 'Low', 'Close','Rentabilidad','Volume_normalized','Rentabilidad_normalized','Volume']\n",
    "for columna in columnas_binarizar:\n",
    "    df[columna] = (df[columna].shift(1) < df[columna]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "sjfbTCecIaXj",
    "outputId": "ab37faf8-f579-45dd-abdf-8011ce125363"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "jO4Zf96UJSjm",
    "outputId": "18b5d6da-e053-4d25-d174-cecce8b874af"
   },
   "outputs": [],
   "source": [
    "# Esta vale para chequear si est\u00e1 o no bien.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sERy3LEsLwhW",
    "outputId": "0e046634-243e-4457-a04c-295feb9bd99a"
   },
   "outputs": [],
   "source": [
    "''' # Seleccionar el rango de fechas para el IT\n",
    "it_start_date = start_date\n",
    "it_end_date = end  # Ajusta las fechas seg\u00fan tus necesidades\n",
    " '''\n",
    "# Seleccionar los datos para el IT\n",
    "df_it_normalizado = data.loc[(data.index >= it_start_date) & (data.index <= it_end_date)]\n",
    "\n",
    "# Dividir el DataFrame en caracter\u00edsticas (X) y la variable objetivo (y)\n",
    "X = df_it_normalizado.drop([\"Rentabilidad\",\"Rentabilidad_normalized\",\"Close\",\"Volume\"], axis=1)\n",
    "y = df_it_normalizado[\"Rentabilidad_normalized\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size_0_2, random_state=random_seed)\n",
    "\n",
    "# Dividir los datos temporales en conjuntos de validaci\u00f3n y prueba\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_val, random_state=random_seed)\n",
    "\n",
    "# Imprimir el n\u00famero de filas en cada conjunto\n",
    "print(\"IT DataSet has {} rows\".format(len(df_it)))\n",
    "print(\"X_train has {} rows\".format(len(X_train)))\n",
    "print(\"X_val has {} rows\".format(len(X_val)))\n",
    "print(\"X_test has {} rows\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTPdPQVZzFZ6"
   },
   "source": [
    "## En esta celda siguiente, probamos el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "H-GCjDUdRfif",
    "outputId": "68534281-1635-4f4d-9be6-6981b07ee3bf"
   },
   "outputs": [],
   "source": [
    "# Supongamos que ya tienes los conjuntos X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# Definir los hiperpar\u00e1metros que deseas ajustar\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Inicializar la b\u00fasqueda de cuadr\u00edcula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_xgb, cv=cross_validation, n_jobs= n_jobs, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar la b\u00fasqueda de cuadr\u00edcula en los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperpar\u00e1metros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Los mejores hiperpar\u00e1metros encontrados son:\", best_params)\n",
    "\n",
    "# Utilizar el modelo con los mejores hiperpar\u00e1metros para hacer predicciones\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el MSE utilizando el mejor modelo\n",
    "mse_test_XGB_GS = mean_squared_error(y_test, y_pred)\n",
    "print(\"El Mean Squared Error (MSE) utilizando los mejores hiperpar\u00e1metros es:\", mse_test_XGB_GS)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "next_day_prediction = best_model.predict(X_test)\n",
    "\n",
    "# Hacer la predicci\u00f3n para el siguiente d\u00eda\n",
    "print(\"La predicci\u00f3n para el siguiente d\u00eda es:\", next_day_prediction[0])\n",
    "\n",
    "# Desnormalizar la predicci\u00f3n\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# Ajustar el escalador a los valores originales de y_train (esto deber\u00eda haberse hecho previamente)\n",
    "scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Desnormalizar la predicci\u00f3n\n",
    "next_day_prediction_descaled = scaler.inverse_transform(next_day_prediction.reshape(-1, 1))[0][0]\n",
    "print(\"La predicci\u00f3n desnormalizada para el siguiente d\u00eda es:\", next_day_prediction_descaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yG3dMz4zNZH"
   },
   "source": [
    "## Aqui vamos a probar la validaci\u00f3n, convirtiendo el test en datos de entrenamiento tambi\u00e9n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwWbWgweaxKr"
   },
   "outputs": [],
   "source": [
    "# Supongamos que ya tienes los conjuntos X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# Concatenar X_train y X_test para usar todo el conjunto de entrenamiento\n",
    "X_full_train = pd.concat([X_train, X_test])\n",
    "y_full_train = pd.concat([y_train, y_test])\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Inicializar la b\u00fasqueda de cuadr\u00edcula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_xgb, cv=cross_validation, n_jobs=n_jobs, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar la b\u00fasqueda de cuadr\u00edcula en los datos de entrenamiento completo\n",
    "grid_search.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Obtener los mejores hiperpar\u00e1metros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Los mejores hiperpar\u00e1metros encontrados son:\", best_params)\n",
    "\n",
    "# Utilizar el modelo con los mejores hiperpar\u00e1metros para hacer predicciones en el conjunto de validaci\u00f3n\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calcular el MSE utilizando el mejor modelo en el conjunto de validaci\u00f3n\n",
    "mse_val_XGB_GS = mean_squared_error(y_val, y_val_pred)\n",
    "print(\"El Mean Squared Error (MSE) en el conjunto de validaci\u00f3n es:\", mse_val_XGB_GS)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "next_day_val_prediction = best_model.predict(X_val)\n",
    "print(\"La predicci\u00f3n normalizada para el siguiente d\u00eda en el conjunto de validaci\u00f3n es:\", next_day_val_prediction[0])\n",
    "\n",
    "# Desnormalizar la predicci\u00f3n\n",
    "# Ajustar el escalador a los valores originales de y_full_train (esto deber\u00eda haberse hecho previamente)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_full_train_reshaped = y_full_train.values.reshape(-1, 1)  # Aseg\u00farate de que los datos est\u00e1n en el formato correcto\n",
    "scaler.fit(y_full_train_reshaped)\n",
    "\n",
    "# Desnormalizar la predicci\u00f3n\n",
    "next_day_val_prediction_reshaped = next_day_val_prediction.reshape(-1, 1)\n",
    "next_day_val_prediction_descaled = scaler.inverse_transform(next_day_val_prediction_reshaped)[0][0]\n",
    "print(\"La predicci\u00f3n desnormalizada para el siguiente d\u00eda en el conjunto de validaci\u00f3n es:\", next_day_val_prediction_descaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiTyGEk9x6bY"
   },
   "source": [
    "## Divisi\u00f3n Binarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QS8BT1D6hJXs"
   },
   "outputs": [],
   "source": [
    "# Seleccionar los datos para el IT\n",
    "df_it_binarizado = df.loc[(df.index >= it_start_date) & (df.index <= it_end_date)]\n",
    "\n",
    "# Dividir el DataFrame en caracter\u00edsticas (X) y la variable objetivo (y)\n",
    "X = df_it_binarizado.drop([\"Rentabilidad\", \"Rentabilidad_normalized\", \"Close\", \"Volume\"], axis=1)\n",
    "y = df_it_binarizado[\"Rentabilidad_normalized\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size_0_2, random_state=random_seed)\n",
    "\n",
    "# Dividir los datos temporales en conjuntos de validaci\u00f3n y prueba\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_val, random_state=random_seed)\n",
    "\n",
    "# Imprimir el n\u00famero de filas en cada conjunto\n",
    "print(\"IT DataSet has {} rows\".format(len(df_it)))\n",
    "print(\"X_train has {} rows\".format(len(X_train)))\n",
    "print(\"X_val has {} rows\".format(len(X_val)))\n",
    "print(\"X_test has {} rows\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW4MsrvTzaC3"
   },
   "source": [
    "## Aqui vamos a probar el test del Binarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVVHAlbGhit9"
   },
   "outputs": [],
   "source": [
    "# Supongamos que ya tienes los conjuntos X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# Definir los hiperpar\u00e1metros que deseas ajustar\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Inicializar la b\u00fasqueda de cuadr\u00edcula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_xgb, cv=cross_validation, n_jobs=n_jobs, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar la b\u00fasqueda de cuadr\u00edcula en los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperpar\u00e1metros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Los mejores hiperpar\u00e1metros encontrados son:\", best_params)\n",
    "\n",
    "# Utilizar el modelo con los mejores hiperpar\u00e1metros para hacer predicciones\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el MSE utilizando el mejor modelo\n",
    "mse_test_XGB_GS = mean_squared_error(y_test, y_pred)\n",
    "print(\"El Mean Squared Error (MSE) utilizando los mejores hiperpar\u00e1metros es:\", mse_test_XGB_GS)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "next_day_prediction = best_model.predict(X_test)\n",
    "print(\"La predicci\u00f3n para el siguiente d\u00eda es:\", next_day_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-fQzydxVvJz"
   },
   "outputs": [],
   "source": [
    "''' # Suponiendo que ya tienes las predicciones (y_pred) y los valores reales (y_test)\n",
    "# Calcular el AUC y la curva de ROC\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f'AUC: {roc_auc:.2f}')\n",
    "\n",
    "# Calcular la curva de ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Graficar la curva de ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WZXVtreRig3"
   },
   "outputs": [],
   "source": [
    "# Convertir las predicciones continuas en clases binarias\n",
    "y_pred_binary = np.where(y_pred > threshold_non_scaled, 1, 0)\n",
    "\n",
    "# Calcular la matriz de confusi\u00f3n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Matriz de Confusi\u00f3n:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calcular verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos\n",
    "true_positives = conf_matrix[1, 1]\n",
    "true_negatives = conf_matrix[0, 0]\n",
    "false_positives = conf_matrix[0, 1]\n",
    "false_negatives = conf_matrix[1, 0]\n",
    "print('\\n')\n",
    "# Imprimir los resultados\n",
    "print(\"Verdaderos Positivos:\", true_positives)\n",
    "print(\"Verdaderos Negativos:\", true_negatives)\n",
    "print(\"Falsos Positivos:\", false_positives)\n",
    "print(\"Falsos Negativos:\", false_negatives)\n",
    "print('\\n')\n",
    "# Calcular el accuracy\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(\"Accuracy (%):\", f\"{accuracy_percentage:.2f}\")\n",
    "# Calcular la precisi\u00f3n\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "print(\"Precision:\", precision)\n",
    "precision_percentage = precision * 100\n",
    "print(\"Precision (%):\", f\"{precision_percentage:.2f}\")\n",
    "# Calcular el recall (sensibilidad)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "print(\"Recall (Sensibilidad):\", recall)\n",
    "recall_percentage = recall * 100\n",
    "print(\"Recall (Sensibilidad) (%):\", f\"{recall_percentage:.2f}\")\n",
    "# Calcular la especificidad\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "print(\"Specificity:\", specificity)\n",
    "specificity_percentage = specificity * 100\n",
    "print(\"Specificity (%):\", f\"{specificity_percentage:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4dcky2wzjg0"
   },
   "source": [
    "## Aqui probamos la validaci\u00f3n del Binarizado, convirtiendo los datos de test en datos de entrenamiento tambi\u00e9n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UYmCMGohpyM"
   },
   "outputs": [],
   "source": [
    "# Supongamos que ya tienes los conjuntos X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# Concatenar X_train y X_test para usar todo el conjunto de entrenamiento\n",
    "X_full_train = pd.concat([X_train, X_test])\n",
    "y_full_train = pd.concat([y_train, y_test])\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Inicializar la b\u00fasqueda de cuadr\u00edcula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid_xgb, cv=cross_validation, n_jobs=n_jobs, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Ejecutar la b\u00fasqueda de cuadr\u00edcula en los datos de entrenamiento completo\n",
    "grid_search.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Obtener los mejores hiperpar\u00e1metros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Los mejores hiperpar\u00e1metros encontrados son:\", best_params)\n",
    "\n",
    "# Utilizar el modelo con los mejores hiperpar\u00e1metros para hacer predicciones en el conjunto de validaci\u00f3n\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calcular el MSE utilizando el mejor modelo en el conjunto de validaci\u00f3n\n",
    "mse_val_XGB_GS = mean_squared_error(y_val, y_val_pred)\n",
    "print(\"El Mean Squared Error (MSE) en el conjunto de validaci\u00f3n es:\", mse_val_XGB_GS)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "next_day_val_prediction = best_model.predict(X_val)\n",
    "\n",
    "print(\"La predicci\u00f3n normalizada para el siguiente d\u00eda en el conjunto de validaci\u00f3n es:\", next_day_val_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdAyjs0zXBse"
   },
   "outputs": [],
   "source": [
    "# Supongamos que ya tienes las predicciones (y_val_pred) y las etiquetas verdaderas (y_val)\n",
    "# Calcular el AUC\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f'AUC: {roc_auc:.2f}')\n",
    "\n",
    "# Calcular la curva de ROC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_pred)\n",
    "\n",
    "# Graficar la curva de ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yi3WLnTJUhi"
   },
   "outputs": [],
   "source": [
    "# Definir el umbral para convertir las predicciones continuas en clases binarias\n",
    "threshold_non_scaled = 0.5  # Ajusta este valor seg\u00fan sea necesario\n",
    "\n",
    "# Convertir las predicciones continuas en clases binarias\n",
    "y_val_pred_binary = np.where(y_val_pred > threshold_non_scaled, 1, 0)\n",
    "\n",
    "# Calcular la matriz de confusi\u00f3n\n",
    "conf_matrix_val = confusion_matrix(y_val, y_val_pred_binary)\n",
    "\n",
    "# Crear el gr\u00e1fico de la matriz de confusi\u00f3n\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            annot_kws={\"size\": 16}, linewidths=1.5, linecolor='White')\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.title('Matriz de Confusi\u00f3n en el conjunto de validaci\u00f3n', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Calcular e imprimir las m\u00e9tricas\n",
    "true_positives_val = conf_matrix_val[1, 1]\n",
    "true_negatives_val = conf_matrix_val[0, 0]\n",
    "false_positives_val = conf_matrix_val[0, 1]\n",
    "false_negatives_val = conf_matrix_val[1, 0]\n",
    "\n",
    "accuracy_val = (true_positives_val + true_negatives_val) / (true_positives_val + true_negatives_val + false_positives_val + false_negatives_val)\n",
    "precision_val = true_positives_val / (true_positives_val + false_positives_val)\n",
    "recall_val = true_positives_val / (true_positives_val + false_negatives_val)\n",
    "specificity_val = true_negatives_val / (true_negatives_val + false_positives_val)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_val:.2f}\")\n",
    "print(f\"Precision: {precision_val:.2f}\")\n",
    "print(f\"Recall (Sensibilidad): {recall_val:.2f}\")\n",
    "print(f\"Specificity: {specificity_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gi0rG27kd5dX"
   },
   "outputs": [],
   "source": [
    "# Convertir las predicciones continuas en clases binarias\n",
    "y_val_pred_binary = np.where(y_val_pred > threshold_non_scaled, 1, 0)\n",
    "\n",
    "# Calcular la matriz de confusi\u00f3n\n",
    "conf_matrix_val = confusion_matrix(y_val, y_val_pred_binary)\n",
    "\n",
    "print(\"Matriz de Confusi\u00f3n en el conjunto de validaci\u00f3n:\")\n",
    "print(conf_matrix_val)\n",
    "print('\\n')\n",
    "\n",
    "# Calcular verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos\n",
    "true_positives_val = conf_matrix_val[1, 1]\n",
    "true_negatives_val = conf_matrix_val[0, 0]\n",
    "false_positives_val = conf_matrix_val[0, 1]\n",
    "false_negatives_val = conf_matrix_val[1, 0]\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Verdaderos Positivos en el conjunto de validaci\u00f3n:\", true_positives_val)\n",
    "print(\"Verdaderos Negativos en el conjunto de validaci\u00f3n:\", true_negatives_val)\n",
    "print(\"Falsos Positivos en el conjunto de validaci\u00f3n:\", false_positives_val)\n",
    "print(\"Falsos Negativos en el conjunto de validaci\u00f3n:\", false_negatives_val)\n",
    "print('\\n')\n",
    "\n",
    "# Calcular el accuracy en porcentaje\n",
    "accuracy_val = (true_positives_val + true_negatives_val) / (true_positives_val + true_negatives_val + false_positives_val + false_negatives_val)\n",
    "accuracy_val_percentage = accuracy_val * 100\n",
    "print(\"Accuracy:\", accuracy_val)\n",
    "print(\"Accuracy(%):\", f\"{accuracy_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular la precisi\u00f3n en porcentaje\n",
    "precision_val = true_positives_val / (true_positives_val + false_positives_val)\n",
    "precision_val_percentage = precision_val * 100\n",
    "print(\"Precision:\", precision_val)\n",
    "print(\"Precision(%):\", f\"{precision_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular el recall (sensibilidad) en porcentaje\n",
    "recall_val = true_positives_val / (true_positives_val + false_negatives_val)\n",
    "recall_val_percentage = recall_val * 100\n",
    "print(\"Recall (Sensibilidad):\", recall_val)\n",
    "print(\"Recall (Sensibilidad)(%):\", f\"{recall_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular la especificidad en porcentaje\n",
    "specificity_val = true_negatives_val / (true_negatives_val + false_positives_val)\n",
    "specificity_val_percentage = specificity_val * 100\n",
    "print(\"Specificity:\", specificity_val)\n",
    "print(\"Specificity(%):\", f\"{specificity_val_percentage:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PceK_1MW4YG"
   },
   "outputs": [],
   "source": [
    "''' import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convertir las predicciones continuas en clases binarias\n",
    "y_val_pred_binary = np.where(y_val_pred > threshold_non_scaled, 1, 0)\n",
    "\n",
    "# Calcular la matriz de confusi\u00f3n\n",
    "conf_matrix_val = confusion_matrix(y_val, y_val_pred_binary)\n",
    "\n",
    "print(\"Matriz de Confusi\u00f3n en el conjunto de validaci\u00f3n:\")\n",
    "print(conf_matrix_val)\n",
    "print('\\n')\n",
    "\n",
    "# Calcular verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos\n",
    "true_positives_val = conf_matrix_val[1, 1]\n",
    "true_negatives_val = conf_matrix_val[0, 0]\n",
    "false_positives_val = conf_matrix_val[0, 1]\n",
    "false_negatives_val = conf_matrix_val[1, 0]\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Verdaderos Positivos en el conjunto de validaci\u00f3n:\", true_positives_val)\n",
    "print(\"Verdaderos Negativos en el conjunto de validaci\u00f3n:\", true_negatives_val)\n",
    "print(\"Falsos Positivos en el conjunto de validaci\u00f3n:\", false_positives_val)\n",
    "print(\"Falsos Negativos en el conjunto de validaci\u00f3n:\", false_negatives_val)\n",
    "print('\\n')\n",
    "\n",
    "# Calcular el accuracy en porcentaje\n",
    "accuracy_val = (true_positives_val + true_negatives_val) / (true_positives_val + true_negatives_val + false_positives_val + false_negatives_val)\n",
    "accuracy_val_percentage = accuracy_val * 100\n",
    "print(\"Accuracy:\", accuracy_val)\n",
    "print(\"Accuracy(%):\", f\"{accuracy_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular la precisi\u00f3n en porcentaje\n",
    "precision_val = true_positives_val / (true_positives_val + false_positives_val)\n",
    "precision_val_percentage = precision_val * 100\n",
    "print(\"Precision:\", precision_val)\n",
    "print(\"Precision(%):\", f\"{precision_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular el recall (sensibilidad) en porcentaje\n",
    "recall_val = true_positives_val / (true_positives_val + false_negatives_val)\n",
    "recall_val_percentage = recall_val * 100\n",
    "print(\"Recall (Sensibilidad):\", recall_val)\n",
    "print(\"Recall (Sensibilidad)(%):\", f\"{recall_val_percentage:.2f}\")\n",
    "\n",
    "# Calcular la especificidad en porcentaje\n",
    "specificity_val = true_negatives_val / (true_negatives_val + false_positives_val)\n",
    "specificity_val_percentage = specificity_val * 100\n",
    "print(\"Specificity:\", specificity_val)\n",
    "print(\"Specificity(%):\", f\"{specificity_val_percentage:.2f}\")\n",
    "\n",
    "# Crear el gr\u00e1fico de la matriz de confusi\u00f3n\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Mostrar la matriz de confusi\u00f3n con los colores personalizados\n",
    "ax.matshow(conf_matrix_val, cmap=plt.cm.Blues)\n",
    "\n",
    "# Asignar colores personalizados a las celdas\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        color = 'green' if (i == j) else 'blue'\n",
    "        ax.text(j, i, format(conf_matrix_val[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_matrix_val[i, j] > conf_matrix_val.max() / 2 else \"black\",\n",
    "                bbox=dict(facecolor=color, edgecolor='none'))\n",
    "\n",
    "ax.set_xticks(np.arange(2))\n",
    "ax.set_yticks(np.arange(2))\n",
    "ax.set_xticklabels(['Negativo', 'Positivo'])\n",
    "ax.set_yticklabels(['Negativo', 'Positivo'])\n",
    "ax.set_xlabel('Predicci\u00f3n')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Matriz de Confusi\u00f3n en el Conjunto de Validaci\u00f3n')\n",
    "\n",
    "plt.show()\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziPZhMZzhrU1"
   },
   "source": [
    "# 6. ESCALADO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u15dVEOMPmVn"
   },
   "source": [
    "## Descarga de Tickers. Filtrado (2). Rentabilidad. Descarga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04jkuj1XxcdA"
   },
   "outputs": [],
   "source": [
    "def get_sp500_listing_dates():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    rows = table.find_all(\"tr\")[1:]  # Exclusi\u00f3n de encabezados\n",
    "\n",
    "    sp500_listing_dates = {}\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all(\"td\")\n",
    "        symbol = data[0].text.strip()\n",
    "        listing_date = data[5].text.strip()\n",
    "        sp500_listing_dates[symbol] = listing_date\n",
    "    return sp500_listing_dates\n",
    "\n",
    "sp500_listing_dates = get_sp500_listing_dates()\n",
    "\n",
    "df_list_of_tickers = pd.DataFrame(sp500_listing_dates.items(), columns=['Empresa', 'Listado_SP500'])\n",
    "df_list_of_tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izGK7UFvyRYI"
   },
   "outputs": [],
   "source": [
    "df_sorted = df_list_of_tickers.sort_values(by='Listado_SP500', ascending=True)\n",
    "df_sorted['Listado_SP500'] = pd.to_datetime(df_sorted['Listado_SP500'])\n",
    "df_sorted['Year'] = df_sorted['Listado_SP500'].dt.year\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vH4VqRlUy_-f"
   },
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame para incluir solo los listados antes a 2001\n",
    "df_filtered = df_sorted[df_sorted['Year'] < 2000]\n",
    "df_filtered.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c449U1Y6y1K-"
   },
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR98cGF83Eey"
   },
   "outputs": [],
   "source": [
    "contains_dot = df_sorted[\"Empresa\"].str.contains(\".\", regex=False)\n",
    "rows_with_dot = df_sorted[contains_dot]\n",
    "\n",
    "if len(rows_with_dot) > 0:\n",
    "    print(\"S\u00ed, hay filas que contienen un '.' en la columna 'Empresa':\")\n",
    "    print(rows_with_dot)\n",
    "else:\n",
    "    print(\"No hay filas que contengan un '.' en la columna 'Empresa'.\")\n",
    "\n",
    "'''\n",
    "En las empresas del S&P 500, las siglas con un \".B\" (o \".A\") indican diferentes clases de acciones para la misma empresa. Las compa\u00f1\u00edas pueden emitir m\u00e1s de una clase de acciones con diferentes derechos de voto, dividendos u otros atributos.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21bkMuVDxlZ5"
   },
   "outputs": [],
   "source": [
    "# Definir la expresi\u00f3n regular para buscar dos o m\u00e1s caracteres en la misma secuencia\n",
    "pattern = r'(\\w)\\1{1,}'\n",
    "\n",
    "# Aplicar la expresi\u00f3n regular a la columna \"Empresa\"\n",
    "contains_sequence = df_filtered[\"Empresa\"].str.contains(pattern, regex=True)\n",
    "\n",
    "# Filtrar las filas que contienen la secuencia\n",
    "rows_with_sequence = df_filtered[contains_sequence]\n",
    "\n",
    "if len(rows_with_sequence) > 0:\n",
    "    print(\"S\u00ed, hay filas que contienen dos o m\u00e1s caracteres en la misma secuencia en la columna 'Empresa':\")\n",
    "    print(rows_with_sequence)\n",
    "else:\n",
    "    print(\"No hay filas que contengan dos o m\u00e1s caracteres en la misma secuencia en la columna 'Empresa'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URn08OvGxlh6"
   },
   "outputs": [],
   "source": [
    "tickers = df_filtered['Empresa'].tolist()\n",
    "\n",
    "# Descargar datos del S&P 500\n",
    "sp500_data = yf.download(tickers, start=start_date, end=end_date)\n",
    "\n",
    "# Asegurarse de que el DataFrame no est\u00e9 vac\u00edo\n",
    "if not sp500_data.empty:\n",
    "    # Crear un diccionario para almacenar los resultados\n",
    "    results = {\n",
    "        'Ticker': [],\n",
    "        'Rentabilidad Anualizada': [],\n",
    "        'Rentabilidad Total': []\n",
    "    }\n",
    "\n",
    "    # Calcular el retorno diario, acumulado y anualizado para cada ticker\n",
    "    for ticker in tickers:\n",
    "        # Calcular el retorno diario\n",
    "        sp500_data['Daily_Return'] = sp500_data['Adj Close'][ticker].pct_change()\n",
    "\n",
    "        # Calcular el retorno acumulado\n",
    "        sp500_data['Cumulative_Return'] = (1 + sp500_data['Daily_Return']).cumprod()\n",
    "\n",
    "        # Calcular el retorno anualizado\n",
    "        total_days = sp500_data.shape[0]\n",
    "        total_years = total_days / 252  # 252 d\u00edas h\u00e1biles en un a\u00f1o\n",
    "        annualized_return = (sp500_data['Cumulative_Return'].iloc[-1])**(1/total_years) - 1\n",
    "\n",
    "        # Agregar los resultados al diccionario\n",
    "        results['Ticker'].append(ticker)\n",
    "        results['Rentabilidad Anualizada'].append(annualized_return)\n",
    "        results['Rentabilidad Total'].append(sp500_data['Cumulative_Return'].iloc[-1])\n",
    "\n",
    "    # Convertir el diccionario en un DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Mostrar el DataFrame resultado\n",
    "    result_df\n",
    "else:\n",
    "    print(\"El DataFrame de datos del S&P 500 est\u00e1 vac\u00edo.\")\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrZh4Z1t0Gu1"
   },
   "outputs": [],
   "source": [
    "# Descargar datos del S&P 500 desde el 1 de enero de 2000 hasta el 31 de diciembre de 2023\n",
    "sp500_data = yf.download('^GSPC', start=start_date, end=end_date)\n",
    "\n",
    "# Calcular el retorno diario\n",
    "sp500_data['Daily_Return'] = sp500_data['Adj Close'].pct_change()\n",
    "\n",
    "# Calcular el retorno acumulado\n",
    "sp500_data['Cumulative_Return'] = (1 + sp500_data['Daily_Return']).cumprod()\n",
    "\n",
    "# Calcular el retorno anualizado\n",
    "total_years = len(sp500_data) / 252  # 252 d\u00edas h\u00e1biles en un a\u00f1o\n",
    "annualized_return = (sp500_data['Cumulative_Return'].iloc[-1])**(1/total_years) - 1\n",
    "\n",
    "# Crear un DataFrame con la rentabilidad anualizada y la rentabilidad total\n",
    "result_df_SP500 = pd.DataFrame({\n",
    "    'Ticker': 'SP_500',\n",
    "    'Rentabilidad Anualizada': [annualized_return],\n",
    "    'Rentabilidad Total': [sp500_data['Cumulative_Return'].iloc[-1]]\n",
    "})\n",
    "\n",
    "result_df_SP500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb7MrKcXxloQ"
   },
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([result_df, result_df_SP500], ignore_index=True)\n",
    "concatenated_df_sorted = concatenated_df.sort_values(by='Rentabilidad Total', ascending=False)\n",
    "concatenated_df_sorted = concatenated_df_sorted.drop(concatenated_df_sorted[concatenated_df_sorted['Ticker'] == 'BF.B'].index)\n",
    "concatenated_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEDIsO6Axls_"
   },
   "outputs": [],
   "source": [
    "# N\u00famero determinado\n",
    "valor_limite = result_df_SP500[\"Rentabilidad Anualizada\"].values[0]\n",
    "\n",
    "# Crear una m\u00e1scara booleana para las filas que cumplen la condici\u00f3n\n",
    "mask = concatenated_df_sorted['Rentabilidad Anualizada'] > valor_limite\n",
    "\n",
    "# Filtrar el DataFrame\n",
    "concatenated_df_sorted_filtrado = concatenated_df_sorted[mask]\n",
    "concatenated_df_sorted_filtrado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqBUtA_ixlw4"
   },
   "outputs": [],
   "source": [
    "df_maestro = concatenated_df_sorted_filtrado\n",
    "df_maestro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF9Ari9Txl1B"
   },
   "outputs": [],
   "source": [
    "# Definir el nombre que quieres buscar\n",
    "nombre_a_buscar = 'AZO'\n",
    "\n",
    "# Filtrar el DataFrame para encontrar el nombre espec\u00edfico\n",
    "resultado = df_maestro[df_maestro['Ticker'] == nombre_a_buscar]\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qs4zaMEZ3JBW"
   },
   "outputs": [],
   "source": [
    "# Supongamos que df_maestro ya est\u00e1 definido y tiene una columna llamada 'TICKER'\n",
    "tickers = df_maestro['Ticker'].tolist()\n",
    "\n",
    "# Diccionario para almacenar los dataframes de cada ticker\n",
    "cotizaciones_dict = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    cotizaciones_dict[ticker] = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Puedes unir todos los dataframes en uno solo si lo deseas\n",
    "cotizaciones_df = pd.concat(cotizaciones_dict, axis=1)\n",
    "cotizaciones_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXG2T9NBbius"
   },
   "source": [
    "## Normalizaci\u00f3n de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP_fs3xxbhrP"
   },
   "outputs": [],
   "source": [
    "# Asegurar que las columnas tengan un MultiIndex\n",
    "if not isinstance(cotizaciones_df.columns, pd.MultiIndex):\n",
    "    cotizaciones_df.columns = pd.MultiIndex.from_tuples(cotizaciones_df.columns, names=['Ticker', 'Attribute'])\n",
    "\n",
    "# Extraer el primer nivel de las columnas que corresponde a los tickers\n",
    "tickers = cotizaciones_df.columns.get_level_values(0).unique().tolist()\n",
    "\n",
    "# Crear la columna Rentabilidad para todas las empresas\n",
    "for ticker in tickers:\n",
    "    open_prices = cotizaciones_df[(ticker, 'Open')]\n",
    "    close_prices = cotizaciones_df[(ticker, 'Close')]\n",
    "    rentabilidad = ((close_prices - open_prices) / open_prices) * 100\n",
    "    cotizaciones_df[(ticker, 'Rentabilidad')] = rentabilidad\n",
    "\n",
    "# Normalizar las columnas Volume y Rentabilidad\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Normalizar la columna Volume\n",
    "    volume = cotizaciones_df[(ticker, 'Volume')].values.reshape(-1, 1)\n",
    "    cotizaciones_df[(ticker, 'Volume_normalized')] = scaler.fit_transform(volume)\n",
    "\n",
    "    # Normalizar la columna Rentabilidad\n",
    "    rentabilidad = cotizaciones_df[(ticker, 'Rentabilidad')].values.reshape(-1, 1)\n",
    "    cotizaciones_df[(ticker, 'Rentabilidad_normalized')] = scaler.fit_transform(rentabilidad)\n",
    "\n",
    "# Ordenar las columnas para que Rentabilidad y las columnas normalizadas est\u00e9n al final de cada subcolumna\n",
    "new_columns = []\n",
    "for ticker in tickers:\n",
    "    sub_columns = cotizaciones_df[ticker].columns.tolist()\n",
    "    sub_columns = [col for col in sub_columns if col not in ['Rentabilidad', 'Volume_normalized', 'Rentabilidad_normalized']]\n",
    "    sub_columns.extend(['Rentabilidad', 'Volume_normalized', 'Rentabilidad_normalized'])\n",
    "    new_columns.extend([(ticker, sub_col) for sub_col in sub_columns])\n",
    "\n",
    "cotizaciones_df = cotizaciones_df[new_columns]\n",
    "cotizaciones_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eddoe2Q7qki0"
   },
   "source": [
    "## Regresi\u00f3n Lineal w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agXIW9NY25q0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Supongamos que df_maestro ya est\u00e1 definido y tiene una columna llamada 'Ticker'\n",
    "tickers = df_maestro['Ticker'].tolist()\n",
    "\n",
    "# Asegurarse de que las columnas 'Date' est\u00e9n de tipo datetime\n",
    "for ticker in tickers:\n",
    "    if 'Date' in cotizaciones_df[ticker].columns:\n",
    "        cotizaciones_df[(ticker, 'Date')] = pd.to_datetime(cotizaciones_df[(ticker, 'Date')])\n",
    "\n",
    "# Definir los par\u00e1metros para GridSearchCV\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'positive': [True, False],\n",
    "    'n_jobs': [None, -1]\n",
    "}\n",
    "\n",
    "# Resultados para los modelos est\u00e1ndar y optimizados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre cada ticker en el primer nivel del MultiIndex con una barra de progreso\n",
    "for ticker in tqdm(tickers, desc=\"Procesando tickers\", unit=\"ticker\"): ## Esto est\u00e1 hecho solo para los dos primeros. Si se quiere hacer de todos simplemente quitar el [;2]\n",
    "    # Filtrar las columnas para el ticker actual\n",
    "    df_ticker = cotizaciones_df[ticker]\n",
    "\n",
    "    # Asegurarse de que los datos est\u00e9n ordenados por fecha\n",
    "    df_ticker = df_ticker.sort_values(by='Date')\n",
    "\n",
    "    # Aqui es donde se elimina la primera fila del df.\n",
    "    df_ticker = df_ticker.iloc[1:]\n",
    "    df_ticker = df_ticker.dropna()\n",
    "    # Aqui es donde se susttuye el valor de la primera fila del y_train con el de la siguiente fila.\n",
    "\n",
    "\n",
    "    # y_train = y_train_sorted.fillna(method='bfill')\n",
    "\n",
    "# Esta linea de arriba esta comentada, si no funciona el modelo, se quite el comentario y se reejecute y se deje as\u00ed\n",
    "\n",
    "\n",
    "    # Modelo de regresi\u00f3n lineal est\u00e1ndar\n",
    "    model_standard = LinearRegression()\n",
    "    model_standard.fit(X_train, y_train)\n",
    "    y_pred_standard = model_standard.predict(X_test)\n",
    "    mse_standard = mean_squared_error(y_test, y_pred_standard)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=LinearRegression(), param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_grid = best_model.predict(X_test)\n",
    "    mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "    # Comparar MSE est\u00e1ndar y MSE con Grid Search\n",
    "    if mse_grid < mse_standard:\n",
    "        comparison = 'Mejor'\n",
    "    elif mse_grid > mse_standard:\n",
    "        comparison = 'Peor'\n",
    "    else:\n",
    "        comparison = 'Igual'\n",
    "\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': best_model,\n",
    "        'Comparacion': comparison,\n",
    "        #'Graficado': y_pred_grid # Esta es para que cuando se ejecute (aunque en este an\u00e1lisis no se han guardado) para poder graficar los histogramas (unicamente por eso)\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_linearregresion = pd.DataFrame(results)\n",
    "df_linearregresion\n",
    "\n",
    "# No tarda tanto como parece, pone 2h 23' pero igual solo es 2h\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sM6bFutyEWD"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': best_model,\n",
    "        'Comparacion': comparison,\n",
    "        #'Graficado': y_pred_grid # Esta es para que cuando se ejecute (aunque en este an\u00e1lisis no se han guardado) para poder graficar los histogramas (unicamente por eso)\n",
    "    })\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_linearregresion = pd.DataFrame(results)\n",
    "df_linearregresion.tail(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb98mZeFKsZ5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Contar los valores de la columna 'Comparison'\n",
    "comparison_counts = df_linearregresion['Comparacion'].value_counts()\n",
    "\n",
    "# Imprimir el conteo de comparaciones\n",
    "print(\"Conteo de comparaciones:\")\n",
    "comparison_counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zV8nYiqvSktk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Suponiendo que el nombre del archivo sea \"df_linearregresion.csv\"\n",
    "df_linearregresion.to_excel(\"df_linearregresion.xlsx\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE2gSpRLrkP6"
   },
   "source": [
    "## Random Forest w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y--D07YrmzS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Definir los par\u00e1metros para GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Resultados para los modelos est\u00e1ndar y optimizados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre cada ticker en el primer nivel del MultiIndex con una barra de progreso\n",
    "for ticker in tqdm(tickers, desc=\"Procesando tickers\", unit=\"ticker\"):\n",
    "    # Filtrar las columnas para el ticker actual\n",
    "    df_ticker = cotizaciones_df[ticker]\n",
    "\n",
    "    # Asegurarse de que los datos est\u00e9n ordenados por fecha\n",
    "    df_ticker = df_ticker.sort_values(by='Date')\n",
    "\n",
    "    # Aqui es donde se elimina la primera fila del df.\n",
    "    df_ticker = df_ticker.iloc[1:]\n",
    "\n",
    "    df_ticker = df_ticker.dropna()\n",
    "\n",
    "    # Modelo de Random Forest est\u00e1ndar\n",
    "    model_standard = RandomForestRegressor(random_state=random_seed)\n",
    "    model_standard.fit(X_train, y_train)\n",
    "    y_pred_standard = model_standard.predict(X_test)\n",
    "    mse_standard = mean_squared_error(y_test, y_pred_standard)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=random_seed), param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_grid = best_model.predict(X_test)\n",
    "    mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "    # Comparar MSE est\u00e1ndar y MSE con Grid Search\n",
    "    if mse_grid < mse_standard:\n",
    "        comparison = 'Mejor'\n",
    "    elif mse_grid > mse_standard:\n",
    "        comparison = 'Peor'\n",
    "    else:\n",
    "        comparison = 'Igual'\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Comparacion': comparison\n",
    "    })\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_randomforest = pd.DataFrame(results)\n",
    "df_randomforest\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmGe3JzSsWTd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Contar los valores de la columna 'Comparacion'\n",
    "comparison_counts = df_randomforest['Comparacion'].value_counts()\n",
    "\n",
    "# Imprimir el conteo de comparaciones\n",
    "print(\"Conteo de comparaciones:\")\n",
    "comparison_counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s9pwjUhXQFD"
   },
   "source": [
    "## Light GMB w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eILtQPKDXS-u"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Supongamos que df_maestro ya est\u00e1 definido y tiene una columna llamada 'Ticker'\n",
    "tickers = df_maestro['Ticker'].tolist()\n",
    "'''\n",
    "'''\n",
    "# Asegurarse de que las columnas 'Date' est\u00e9n de tipo datetime\n",
    "for ticker in tickers:\n",
    "    if 'Date' in cotizaciones_df[ticker].columns:\n",
    "        cotizaciones_df[(ticker, 'Date')] = pd.to_datetime(cotizaciones_df[(ticker, 'Date')])\n",
    "'''\n",
    " '''\n",
    "# Definir los par\u00e1metros para GridSearchCV\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Resultados para los modelos est\u00e1ndar y optimizados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre cada ticker en el primer nivel del MultiIndex con una barra de progreso\n",
    "for ticker in tqdm(tickers, desc=\"Procesando tickers\", unit=\"ticker\"):\n",
    "    # Filtrar las columnas para el ticker actual\n",
    "    df_ticker = cotizaciones_df[ticker]\n",
    "\n",
    "    # Asegurarse de que los datos est\u00e9n ordenados por fecha\n",
    "    df_ticker = df_ticker.sort_values(by='Date')\n",
    "\n",
    "    # Aqui es donde se elimina la primera fila del df.\n",
    "    df_ticker = df_ticker.iloc[1:]\n",
    "\n",
    "    df_ticker = df_ticker.dropna()\n",
    "\n",
    "    # Rellenar NaNs en y_train con el valor del siguiente dato\n",
    "    y_train = y_train.fillna(method='bfill')\n",
    "\n",
    "    # Modelo de LightGBM est\u00e1ndar\n",
    "    model_standard = LGBMRegressor()\n",
    "    model_standard.fit(X_train, y_train)\n",
    "    y_pred_standard = model_standard.predict(X_test)\n",
    "    mse_standard = mean_squared_error(y_test, y_pred_standard)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=LGBMRegressor(), param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_grid = best_model.predict(X_test)\n",
    "    mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "    # Comparar MSE est\u00e1ndar y MSE con Grid Search\n",
    "    if mse_grid < mse_standard:\n",
    "        comparison = 'Mejor'\n",
    "    elif mse_grid > mse_standard:\n",
    "        comparison = 'Peor'\n",
    "    else:\n",
    "        comparison = 'Igual'\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Comparacion': comparison\n",
    "    })\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_lgbm_results = pd.DataFrame(results)\n",
    "df_lgbm_results\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7uqvSTUXTT1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Contar los valores de la columna 'Comparacion'\n",
    "comparison_counts = df_lgbm_results['Comparacion'].value_counts()\n",
    "\n",
    "# Imprimir el conteo de comparaciones\n",
    "print(\"Conteo de comparaciones:\")\n",
    "comparison_counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XInchLnjWfrx"
   },
   "source": [
    "## XGBOOST w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2CBjn6HWu96"
   },
   "outputs": [],
   "source": [
    "tickers = df_maestro['Ticker'].tolist()\n",
    "\n",
    "# Resultados para los modelos est\u00e1ndar y optimizados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre cada ticker en el primer nivel del MultiIndex. TQDM mide el progreso del c\u00e1lculo realizado.\n",
    "for ticker in tqdm(tickers, desc=\"Procesando tickers\", unit=\"ticker\"):\n",
    "\n",
    "    # Filtrar las columnas para el ticker actual\n",
    "    df_ticker = cotizaciones_df[ticker]\n",
    "\n",
    "    # Asegurarse de que los datos est\u00e9n ordenados por fecha\n",
    "    df_ticker = df_ticker.sort_values(by='Date')\n",
    "\n",
    "    # Filtrados y eliminaciones para poder ejecutar correctamente el bucle.\n",
    "    df_ticker = df_ticker.iloc[1:]\n",
    "    df_ticker = df_ticker.dropna()\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba de forma iterativa para cada Ticker\n",
    "    columns_to_drop = [\"Rentabilidad\", \"Rentabilidad_normalized\", \"Adj Close\", \"Volume\"]\n",
    "    X = df_ticker.drop(columns=columns_to_drop)\n",
    "    y = df_ticker[\"Rentabilidad_normalized\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # Modelo de XGBoost est\u00e1ndar\n",
    "    model_standard = XGBRegressor()\n",
    "    model_standard.fit(X_train, y_train)\n",
    "    y_pred_standard = model_standard.predict(X_test)\n",
    "    mse_standard = mean_squared_error(y_test, y_pred_standard)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=XGBRegressor(), param_grid=param_grid_xgb,\n",
    "                               scoring='neg_mean_squared_error', cv=cross_validation, n_jobs=n_jobs, verbose=verbose)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_grid = best_model.predict(X_test)\n",
    "    mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "    # Comparar MSE est\u00e1ndar y MSE con Grid Search\n",
    "    if mse_grid < mse_standard:\n",
    "        comparison = 'Mejor'\n",
    "    elif mse_grid > mse_standard:\n",
    "        comparison = 'Peor'\n",
    "    else:\n",
    "        comparison = 'Igual'\n",
    "\n",
    "    # Hacer la predicci\u00f3n para el siguiente d\u00eda\n",
    "    next_day_prediction = best_model.predict(X_test)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'Comparacion': comparison,\n",
    "        'Next Day Prediction': next_day_prediction[0]\n",
    "    })\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_xgb_results = pd.DataFrame(results)\n",
    "df_xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEdlweLrXNef"
   },
   "outputs": [],
   "source": [
    "comparison_counts = df_xgb_results['Comparacion'].value_counts()\n",
    "\n",
    "# Imprimir el conteo de comparaciones\n",
    "print(\"Conteo de comparaciones:\")\n",
    "comparison_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRJpZicKcoJs"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que el nombre del archivo sea \"df_linearregresion.csv\"\n",
    "# df_xgb_results.to_excel(\"df_xgb_results.xlsx\", index=False)\n",
    "ruta_archivo = r'C:\\Users\\migue\\Downloads\\resultados_xgb.xlsx'\n",
    "df_xgb_results.to_excel(ruta_archivo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpNw4qn3kKKC"
   },
   "outputs": [],
   "source": [
    "\"\"\" # Desnormalizar las rentabilidades normalizadas que se han hecho\n",
    "# Supongamos que y_pred_standard y y_test son las predicciones y las etiquetas verdaderas, respectivamente\n",
    "\n",
    "# Desnormalizar las predicciones\n",
    "y_pred_standard_desnorm = scalers[ticker].inverse_transform(y_pred_standard.reshape(-1, 1))\n",
    "\n",
    "# Desnormalizar las etiquetas verdaderas\n",
    "y_test_desnorm = scalers[ticker].inverse_transform(y_test.values.reshape(-1, 1)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG3ZYoZ8doV5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLoJyZLvdUxZ"
   },
   "source": [
    "## Binarizado (si adaptamos el codigo sin binarizar, para poder usarlo en el binarizado, deber\u00eda realizar la prediccion de t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkeZjSuLdURs"
   },
   "outputs": [],
   "source": [
    "tickers = df_maestro['Ticker'].tolist()\n",
    "\n",
    "# Resultados para los modelos est\u00e1ndar y optimizados\n",
    "results = []\n",
    "\n",
    "# Iterar sobre cada ticker en el primer nivel del MultiIndex con una barra de progreso\n",
    "for ticker in tqdm(tickers, desc=\"Procesando tickers\", unit=\"ticker\"):\n",
    "    # Filtrar las columnas para el ticker actual\n",
    "    df_ticker = cotizaciones_df[ticker]\n",
    "\n",
    "    # Asegurarse de que los datos est\u00e9n ordenados por fecha\n",
    "    df_ticker = df_ticker.sort_values(by='Date')\n",
    "\n",
    "    # Filtrados y eliminaciones para poder ejecutar correctamente el bucle.\n",
    "    df_ticker = df_ticker.iloc[1:]\n",
    "    df_ticker = df_ticker.dropna()\n",
    "\n",
    "    # Definir las columnas a binarizar (todas)\n",
    "    columnas_binarizar = ['Open', 'High', 'Low', 'Close', 'Rentabilidad', 'Volume_normalized', 'Rentabilidad_normalized', 'Adj Close', 'Volume']\n",
    "    for columna in columnas_binarizar:\n",
    "        df_ticker[columna] = (df_ticker[columna].shift(1) < df_ticker[columna]).astype(int)\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba para cada ticker.\n",
    "    columns_to_drop = [\"Rentabilidad\", \"Rentabilidad_normalized\", \"Adj Close\", \"Volume\"]\n",
    "    X = df_ticker.drop(columns=columns_to_drop)\n",
    "    y = df_ticker[\"Rentabilidad_normalized\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # Modelo de XGBoost est\u00e1ndar\n",
    "    model_standard = XGBRegressor()\n",
    "    model_standard.fit(X_train, y_train)\n",
    "    y_pred_standard = model_standard.predict(X_test)\n",
    "    mse_standard = mean_squared_error(y_test, y_pred_standard)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=XGBRegressor(), param_grid=param_grid_xgb,\n",
    "                               scoring='neg_mean_squared_error', cv=cross_validation, n_jobs=n_jobs, verbose=verbose)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_grid = best_model.predict(X_test)\n",
    "    mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "    # ROC y AUC para el modelo optimizado\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grid)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Hacer la predicci\u00f3n para el siguiente d\u00eda utilizando todo el conjunto de datos de prueba\n",
    "    next_day_predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE_Standard': mse_standard,\n",
    "        'MSE_Grid': mse_grid,\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'ROC_AUC': roc_auc,\n",
    "        'Next Day Predictions': next_day_predictions[0]\n",
    "    })\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "df_xgb_results_binarizado = pd.DataFrame(results)\n",
    "df_xgb_results_binarizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFusRsVvoPpi"
   },
   "outputs": [],
   "source": [
    "# df_xgb_results_binarizado.to_excel(\"df_xgb_results_binarizado.xlsx\", index=False)\n",
    "ruta_archivo = r'C:\\Users\\migue\\Downloads\\resultados_xgb_binarizado.xlsx'\n",
    "df_xgb_results_binarizado.to_excel(ruta_archivo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5Awl45mdT3_"
   },
   "outputs": [],
   "source": [
    "# Contraste de hipotesis y confirmaci\u00f3n o rechazo para todas --> sacar el que mejor AUC-ROC tenga (igual de los 138 predice muy bien 3) pues se comprueba esos tres Y SE AFIRMA: ___________________\n",
    "\n",
    "# Podemos decir: guardame la prediccion hecha y me descargas los datos del 2 de enero de 2024 (y chequeamos simplemente de forma visual si tiene o no razon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ojb4ZLjXN3lz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n_ov1o9N3o2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hw-57Fm0N3rt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSKJeIcSN3un"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8qpCvyMN3xT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxjLsYHLN30N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMFplJHSN33a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn23YdSoN36V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75anujpLN5WA"
   },
   "source": [
    "# Graficaci\u00f3n del BXGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLA1oYBiN39M"
   },
   "outputs": [],
   "source": [
    "# Ruta del archivo\n",
    "ruta_archivo = '/content/drive/MyDrive/MASTER/TRABAJO FIN DE MASTER/resultados_xgb (1).xlsx'\n",
    "\n",
    "# Leer el archivo Excel\n",
    "df_xgb_results = pd.read_excel(ruta_archivo)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar\n",
    "df_xgb_results.head()\n",
    "\n",
    "rentabilidad_real = scaler.inverse_transform(df_xgb_results[['Next Day Prediction']])\n",
    "\n",
    "# Agregar la columna desnormalizada al DataFrame\n",
    "df_xgb_results['Rentabilidad Real Predicha'] = rentabilidad_real\n",
    "df_xgb_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtDSFPKWdskI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo\n",
    "ruta_archivo = '/content/drive/MyDrive/MASTER/TRABAJO FIN DE MASTER/df_xgb_results_binarizado_FINALISIMO.xlsx'\n",
    "\n",
    "# Leer el archivo Excel\n",
    "df_xgb_results_binarizado = pd.read_excel(ruta_archivo)\n",
    "\n",
    "# Limitar los valores de la columna 'Next Day Predictions' entre 0 y 1\n",
    "df_xgb_results_binarizado['Next Day Predictions'] = df_xgb_results_binarizado['Next Day Predictions'].clip(lower=0, upper=1)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar\n",
    "df_xgb_results_binarizado.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbqzTYJ7ie20"
   },
   "outputs": [],
   "source": [
    "# Lista de tickers disponibles en la columna 'Ticker'\n",
    "tickers = df_maestro['Ticker'].tolist()\n",
    "\n",
    "# Diccionario para almacenar los dataframes de cada ticker\n",
    "cotizaciones_dict_contraste = {}\n",
    "\n",
    "# Iterar sobre cada ticker y calcular la rentabilidad\n",
    "for ticker in tickers:\n",
    "    # Descargar solo las columnas 'Open' y 'Close' para el ticker y las fechas especificadas\n",
    "    final_df = yf.download(ticker, start=end_date, end=next_day)[['Open', 'Close']]\n",
    "\n",
    "    # Calcular la rentabilidad\n",
    "    open_prices = final_df['Open']\n",
    "    close_prices = final_df['Close']\n",
    "    rentabilidad = ((close_prices - open_prices) / open_prices) * 100\n",
    "\n",
    "    # Crear un nuevo DataFrame para el ticker actual con las columnas 'Ticker' y 'Rentabilidad'\n",
    "    df_ticker = pd.DataFrame({'Ticker': [ticker], 'Rentabilidad': [rentabilidad]})\n",
    "\n",
    "    # Agregar el DataFrame del ticker al diccionario\n",
    "    cotizaciones_dict_contraste[ticker] = df_ticker\n",
    "\n",
    "# Concatenar todos los dataframes de los tickers en uno solo\n",
    "cotizaciones_df_contraste = pd.concat(cotizaciones_dict_contraste.values(), ignore_index=True)\n",
    "cotizaciones_df_contraste['Rentabilidad'] = cotizaciones_df_contraste['Rentabilidad'].astype(str).str.slice(15, -15)\n",
    "df_contraste = cotizaciones_df_contraste\n",
    "df_contraste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2nBguGYmqBT"
   },
   "source": [
    "## Graficaci\u00f3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3cgEm9E0kAd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Funci\u00f3n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Supongamos que tienes los DataFrames ya cargados\n",
    "# Convertir las columnas a tipo num\u00e9rico\n",
    "df_contraste['Rentabilidad'] = pd.to_numeric(df_contraste['Rentabilidad'], errors='coerce')\n",
    "df_xgb_results['Rentabilidad Real Predicha'] = pd.to_numeric(df_xgb_results['Rentabilidad Real Predicha'], errors='coerce')\n",
    "\n",
    "# Suponiendo que tienes un DataFrame llamado df_xgb_results_binarizado con la columna \"Next Day Predictions\" y \"Ticker\"\n",
    "predictions = df_xgb_results_binarizado['Next Day Predictions']\n",
    "tickers = df_xgb_results_binarizado['Ticker']\n",
    "\n",
    "# Normalizar los valores de \"Next Day Predictions\" al rango [0, 1]\n",
    "predictions_normalized = (predictions - predictions.min()) / (predictions.max() - predictions.min())\n",
    "\n",
    "# Crear el rango de valores para x de la sigmoide\n",
    "x_values = np.linspace(-5, 5, 138)\n",
    "\n",
    "# Calcular los valores de la sigmoide para el rango de x\n",
    "sigmoid_values = sigmoid(x_values)\n",
    "\n",
    "# Crear el gr\u00e1fico interactivo con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Definir las condiciones para TP y TN basadas en colores\n",
    "# Verde (True Ups) ser\u00e1n TP y Morado (True Downs) ser\u00e1n TN\n",
    "# Suponemos que las predicciones de rentabilidad real est\u00e1n en df_xgb_results['Rentabilidad Real Predicha']\n",
    "# y las rentabilidades reales est\u00e1n en df_contraste['Rentabilidad']\n",
    "TP_condition = (df_contraste['Rentabilidad'] > 0) & (df_xgb_results['Rentabilidad Real Predicha'] > 0)\n",
    "TN_condition = (df_contraste['Rentabilidad'] <= 0) & (df_xgb_results['Rentabilidad Real Predicha'] <= 0)\n",
    "FP_condition = (df_contraste['Rentabilidad'] <= 0) & (df_xgb_results['Rentabilidad Real Predicha'] > 0)\n",
    "FN_condition = (df_contraste['Rentabilidad'] > 0) & (df_xgb_results['Rentabilidad Real Predicha'] <= 0)\n",
    "\n",
    "# Contar los TP, TN, FP y FN\n",
    "TP_count = TP_condition.sum()\n",
    "TN_count = TN_condition.sum()\n",
    "FP_count = FP_condition.sum()\n",
    "FN_count = FN_condition.sum()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"N\u00famero de Verdaderos Positivos (TP):\", TP_count)\n",
    "print(\"N\u00famero de Verdaderos Negativos (TN):\", TN_count)\n",
    "print(\"N\u00famero de Falsos Positivos (FP):\", FP_count)\n",
    "print(\"N\u00famero de Falsos Negativos (FN):\", FN_count)\n",
    "\n",
    "# Agregar los puntos al gr\u00e1fico con la leyenda del ticker\n",
    "x_values_plot = []\n",
    "y_values_plot = []\n",
    "text_values = []\n",
    "colors = []\n",
    "\n",
    "# Supongamos que los \u00edndices de los DataFrames coinciden y est\u00e1n alineados correctamente\n",
    "for i in range(len(tickers)):\n",
    "    x_values_plot.append(i)\n",
    "    y_values_plot.append(predictions_normalized.iloc[i])\n",
    "    text_values.append(tickers.iloc[i])\n",
    "\n",
    "    # Obtener los valores de rentabilidad y rentabilidad real predicha\n",
    "    rentabilidad = df_contraste['Rentabilidad'].iloc[i]\n",
    "    rentabilidad_real_predicha = df_xgb_results['Rentabilidad Real Predicha'].iloc[i]\n",
    "\n",
    "    # Determinar el color del punto\n",
    "    if (rentabilidad < 0 and rentabilidad_real_predicha > 0):\n",
    "        colors.append('red')  # Rojo para Falsos positivos\n",
    "    elif rentabilidad < 0 and rentabilidad_real_predicha < 0:\n",
    "        colors.append('purple')  # Morado para ambos negativos (True Downs)\n",
    "    elif rentabilidad > 0 and rentabilidad_real_predicha > 0:\n",
    "        colors.append('green')  # Verde para ambos positivos (True Ups)\n",
    "    else:\n",
    "        colors.append('blue')  # Azul para el resto de los casos\n",
    "\n",
    "# Agregar los puntos al gr\u00e1fico\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_values_plot,\n",
    "    y=y_values_plot,\n",
    "    mode='markers',\n",
    "    marker=dict(color=colors),\n",
    "    text=text_values,  # Usar el ticker como texto al pasar el cursor\n",
    "    hoverinfo='text',  # Mostrar el texto al pasar el cursor\n",
    "    name='Next Day Predictions'\n",
    "))\n",
    "\n",
    "# Agregar la l\u00ednea de la funci\u00f3n sigmoide al gr\u00e1fico\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(sigmoid_values))),\n",
    "    y=sigmoid_values,\n",
    "    mode='lines',\n",
    "    legendgroup='group3',\n",
    "    showlegend=True,\n",
    "    name='True Ups'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='blue'),\n",
    "    legendgroup='group4',\n",
    "    showlegend=True,\n",
    "    name='False Negatives'\n",
    "))\n",
    "\n",
    "# Mostrar el gr\u00e1fico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUygLOTvaU1U"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir los recuentos de TP, TN, FP y FN\n",
    "TP = TP_count\n",
    "TN = TN_count\n",
    "FP = FP_count\n",
    "FN = FN_count\n",
    "\n",
    "# Crear la matriz de confusi\u00f3n\n",
    "confusion_matrix = [[TN, FP],\n",
    "                    [FN, TP]]\n",
    "\n",
    "# Definir las etiquetas para los ejes\n",
    "labels = ['Negativo', 'Positivo']\n",
    "\n",
    "# Crear la matriz de confusi\u00f3n con colores utilizando Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.set(font_scale=1.2)  # Ajustar el tama\u00f1o de fuente\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar_kws={'label': 'Recuento de casos'})\n",
    "plt.title('Matriz de Confusi\u00f3n')\n",
    "plt.xlabel('Predicci\u00f3n')\n",
    "plt.ylabel('Real')\n",
    "plt.xticks([0.5, 1.5], labels)\n",
    "plt.yticks([0.5, 1.5], labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FKzR8Oj0kDw"
   },
   "outputs": [],
   "source": [
    "# Funci\u00f3n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Convertir las columnas a tipo num\u00e9rico\n",
    "df_contraste['Rentabilidad'] = pd.to_numeric(df_contraste['Rentabilidad'], errors='coerce')\n",
    "df_xgb_results['Rentabilidad Real Predicha'] = pd.to_numeric(df_xgb_results['Rentabilidad Real Predicha'], errors='coerce')\n",
    "\n",
    "predictions = df_xgb_results_binarizado['Next Day Predictions']\n",
    "tickers = df_xgb_results_binarizado['Ticker']\n",
    "\n",
    "# Crear el rango de valores para x de la sigmoide\n",
    "x_values = np.linspace(-5, 5, len(df_maestro))\n",
    "\n",
    "# Calcular los valores de la sigmoide para el rango de x\n",
    "sigmoid_values = sigmoid(x_values)\n",
    "\n",
    "# Crear el gr\u00e1fico interactivo con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Agregar los puntos al gr\u00e1fico con la leyenda del ticker\n",
    "x_values_plot = []\n",
    "y_values_plot = []\n",
    "text_values = []\n",
    "colors = []\n",
    "threshold_sum = 0  # Variable para calcular el threshold promedio\n",
    "true_ups_count = 0  # Contador de True Ups\n",
    "\n",
    "for i in range(len(tickers)):\n",
    "    x_values_plot.append(i)\n",
    "    y_values_plot.append(predictions_normalized.iloc[i])\n",
    "    text_values.append(tickers.iloc[i])\n",
    "\n",
    "    # Obtener los valores de rentabilidad y rentabilidad real predicha\n",
    "    rentabilidad = df_contraste['Rentabilidad'].iloc[i]\n",
    "    rentabilidad_real_predicha = df_xgb_results['Rentabilidad Real Predicha'].iloc[i]\n",
    "\n",
    "    # Determinar el color del punto\n",
    "    if (rentabilidad < 0 and rentabilidad_real_predicha > 0):\n",
    "        colors.append('red')  # Rojo para Falsos positivos\n",
    "    elif rentabilidad < 0 and rentabilidad_real_predicha < 0:\n",
    "        colors.append('purple')  # Morado para ambos negativos (True Downs)\n",
    "    elif rentabilidad > 0 and rentabilidad_real_predicha > 0:\n",
    "        colors.append('green')  # Verde para ambos positivos (True Ups)\n",
    "        threshold_sum += predictions_normalized.iloc[i]  # Sumar el valor al threshold\n",
    "        true_ups_count += 1  # Incrementar el contador de True Ups\n",
    "    else:\n",
    "        if predictions_normalized.iloc[i] > threshold:\n",
    "            colors.append('green')  # Verde si el valor es mayor que el threshold\n",
    "        else:\n",
    "            colors.append('blue')  # Azul para el resto de los casos\n",
    "\n",
    "# Calcular el threshold promedio\n",
    "threshold = threshold_sum / true_ups_count if true_ups_count > 0 else 0\n",
    "\n",
    "print(\"El threshold \u00f3ptimo basado en la distancia promedio es:\", threshold)\n",
    "\n",
    "# Agregar los puntos al gr\u00e1fico\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_values_plot,\n",
    "    y=y_values_plot,\n",
    "    mode='markers',\n",
    "    marker=dict(color=colors),\n",
    "    text=text_values,  # Usar el ticker como texto al pasar el cursor\n",
    "    hoverinfo='text',  # Mostrar el texto al pasar el cursor\n",
    "    name='Next Day Predictions'\n",
    "))\n",
    "\n",
    "# Agregar la l\u00ednea de la funci\u00f3n sigmoide al gr\u00e1fico\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(sigmoid_values))),\n",
    "    y=sigmoid_values,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=3),\n",
    "    name='Sigmoide'\n",
    "))\n",
    "\n",
    "# Agregar la l\u00ednea punteada para representar el threshold \u00f3ptimo\n",
    "threshold = max(0, min(1, threshold))  # Asegurar que el threshold est\u00e9 en el rango [0, 1]\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=0,\n",
    "    y0=threshold,\n",
    "    x1=len(x_values_plot),\n",
    "    y1=threshold,\n",
    "    line=dict(\n",
    "        color=\"black\",\n",
    "        width=3,\n",
    "        dash=\"dash\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Personalizar el dise\u00f1o del gr\u00e1fico\n",
    "fig.update_layout(\n",
    "    title='Modelo BXGB',\n",
    "    xaxis_title='Ticker',\n",
    "    yaxis_title='Valor',\n",
    "    hoverlabel=dict(bgcolor=\"white\",\n",
    "    font_size=12, font_family=\"Rockwell\"),  # Personalizar el estilo del texto al pasar el cursor\n",
    "    legend=dict(\n",
    "        itemsizing='constant',\n",
    "        title='Leyenda',\n",
    "        font=dict(size=12),\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# A\u00f1adir leyenda manualmente\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='red'),\n",
    "    legendgroup='group1',\n",
    "    showlegend=True,\n",
    "    name='False Positives'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='purple'),\n",
    "    legendgroup='group2',\n",
    "    showlegend=True,\n",
    "    name='True Downs'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='green'),\n",
    "    legendgroup='group3',\n",
    "    showlegend=True,\n",
    "    name='True Ups'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='blue'),\n",
    "    legendgroup='group4',\n",
    "    showlegend=True,\n",
    "    name='False Negatives'\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXjL_d_Af5PY"
   },
   "outputs": [],
   "source": [
    "# Inicializar contadores para cada color\n",
    "red_count = 0\n",
    "purple_count = 0\n",
    "green_count = 0\n",
    "blue_count = 0\n",
    "\n",
    "# Sumar los colores\n",
    "for color in colors:\n",
    "    if color == 'red':\n",
    "        red_count += 1\n",
    "    elif color == 'purple':\n",
    "        purple_count += 1\n",
    "    elif color == 'green':\n",
    "        green_count += 1\n",
    "    elif color == 'blue':\n",
    "        blue_count += 1\n",
    "\n",
    "# Mostrar los totales de cada color\n",
    "print(\"Total de Falsos Positivos (red):\", red_count)\n",
    "print(\"Total de Verdaderos Downs (purple):\", purple_count)\n",
    "print(\"Total de Verdaderos Ups (green):\", green_count)\n",
    "print(\"Total de Falsos Negativos (blue):\", blue_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4761QJlpf5Lm"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir los recuentos de TP, TN, FP y FN\n",
    "TP = green_count\n",
    "TN = purple_count\n",
    "FP = red_count\n",
    "FN = blue_count\n",
    "\n",
    "# Crear la matriz de confusi\u00f3n\n",
    "confusion_matrix = [[TN, FP],\n",
    "                    [FN, TP]]\n",
    "\n",
    "# Definir las etiquetas para los ejes\n",
    "labels = ['Negativo', 'Positivo']\n",
    "\n",
    "# Crear la matriz de confusi\u00f3n con colores utilizando Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.set(font_scale=1.2)  # Ajustar el tama\u00f1o de fuente\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar_kws={'label': 'Recuento de casos'})\n",
    "plt.title('Matriz de Confusi\u00f3n')\n",
    "plt.xlabel('Predicci\u00f3n')\n",
    "plt.ylabel('Real')\n",
    "plt.xticks([0.5, 1.5], labels)\n",
    "plt.yticks([0.5, 1.5], labels)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PfMjSSoej34Y",
    "vZFL5Cna1B5C",
    "kc7I4KZ8qjHy",
    "6jnq3DJvw0UH",
    "ds5p_HRvmYHB",
    "JLGBfd_cRu5A",
    "Q0AdWNpMzTxZ",
    "l0gHFEoKJ3bz",
    "qvIq9nhUeyAI",
    "0k0PP6w75xAu",
    "AvE_dProkz6A",
    "hIvYQcUwy6Hj",
    "_9TnE00HXL8M",
    "2HmaKbHqwcEa",
    "LHx3hLCHl5cn",
    "QNUtENiH0004",
    "O6RFrKN61C1h",
    "bTT4mzIBACk1",
    "JOw1_RzRlvFH",
    "JJxSnA0R52qv",
    "0sxuHzRM6aCz",
    "UwbYcs7K7H1m",
    "-2D1JZyKBWuI",
    "ahLDSVutTad7",
    "Sjkow9wM6Vk1",
    "CTPdPQVZzFZ6",
    "1yG3dMz4zNZH",
    "HiTyGEk9x6bY",
    "sW4MsrvTzaC3",
    "D4dcky2wzjg0",
    "u15dVEOMPmVn",
    "yXG2T9NBbius",
    "eddoe2Q7qki0",
    "aE2gSpRLrkP6",
    "2s9pwjUhXQFD",
    "XInchLnjWfrx",
    "SLoJyZLvdUxZ",
    "75anujpLN5WA",
    "e2nBguGYmqBT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}